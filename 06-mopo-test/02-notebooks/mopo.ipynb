{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9087d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py in /home4/s5231795/.local/lib/python3.10/site-packages (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home4/s5231795/.local/lib/python3.10/site-packages (from h5py) (2.2.6)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/skylake_avx512/software/Python/3.10.4-GCCcore-11.3.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e81ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NN necessities:\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# import plotting utilities:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import data preprocessing utilities:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2718ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):   # class defining a basic nn\n",
    "\n",
    "    def __init__(self, h_size=200):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(23, h_size),      # in\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size, h_size),    # hidden, 3 layers\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(h_size, 18)       # out\n",
    "        )\n",
    "        self.mean_head = nn.Linear(h_size, 18)\n",
    "        self.logvar_head = nn.Linear(h_size, 18)\n",
    "\n",
    "        # bind log-variance to avoid numerical instability\n",
    "        self.max_logvar = nn.Parameter(torch.ones(18) * 0.5)\n",
    "        self.min_logvar = nn.Parameter(torch.ones(18) * -10)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.MSELoss()    # using mean squared error as a loss metric\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.model(x)\n",
    "        mean = self.mean_head(res)\n",
    "        logvar = self.logvar_head(res)\n",
    "\n",
    "        # clamp log-variance using soft constraints (see MBPO/PETS)\n",
    "        logvar = self.max_logvar - torch.nn.functional.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + torch.nn.functional.softplus(logvar - self.min_logvar)\n",
    "        return mean, logvar\n",
    "\n",
    "    def nll_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Negative log-likelihood of Gaussian:\n",
    "            NLL = 0.5 * [ logσ² + (y - µ)² / σ² ]\n",
    "        \"\"\"\n",
    "        mean, logvar = self.forward(x)\n",
    "        var = torch.exp(logvar)\n",
    "\n",
    "        nll = 0.5 * ((y - mean)**2 / var + logvar)\n",
    "        return nll.mean()\n",
    "\n",
    "    def train_epoch(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = self.nll_loss(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def train(self, train_data, epochs=500, cp=100):\n",
    "        x, y = train_data\n",
    "\n",
    "        # again split the data to optimize hyperparam on val set, not leak data.\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "    \n",
    "        test_losses = []\n",
    "        losses = []\n",
    "    \n",
    "        for iter in range(epochs):\n",
    "            # train\n",
    "            iteration_loss = self.train_epoch(x_train, y_train)\n",
    "            losses.append(iteration_loss)\n",
    "\n",
    "            # validate\n",
    "            val_loss = self.validation_loss((x_val, y_val))\n",
    "            test_losses.append(val_loss)\n",
    "    \n",
    "            # print\n",
    "            if iter and iter % cp == 0:    # update on iteration checkpoints\n",
    "                print(f\"iteration {iter}/{epochs}, loss = {iteration_loss}, {val_loss}\")\n",
    "        \n",
    "        return losses, test_losses\n",
    "\n",
    "\n",
    "    def validation_loss(self, test_data):\n",
    "        x, y = test_data\n",
    "        loss = self.nll_loss(x, y)\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69c63b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['actions', 'infos', 'metadata', 'next_observations', 'observations', 'rewards', 'terminals', 'timeouts']>\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = h5py.File(Path(\"./halfcheetah_medium-v2.hdf5\"))\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0b8db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape = (1000000, 6)\n",
      "s shape = (1000000, 17)\n",
      "s_new shape = (1000000, 17)\n",
      "r shape = (1000000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract relevant cols\n",
    "a = data[\"actions\"]\n",
    "s_new = data[\"next_observations\"]\n",
    "s = data[\"observations\"]\n",
    "r = data[\"rewards\"]\n",
    "\n",
    "# info\n",
    "print(\n",
    "    f\"a shape = {a.shape}\\n\" \\\n",
    "    f\"s shape = {s.shape}\\n\" \\\n",
    "    f\"s_new shape = {s_new.shape}\\n\" \\\n",
    "    f\"r shape = {r.shape}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21844542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide data\n",
    "x = np.hstack([a, s])                                # -> (N, 23)\n",
    "y = np.hstack([s_new, np.array(r).reshape(-1, 1)])   # -> (N, 18)\n",
    "\n",
    "# converting to tensors\n",
    "x = torch.tensor(x, dtype=torch.float32)   \n",
    "y = torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42a6fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# your Network class exactly as you gave it above ...\n",
    "# (no need to change it)\n",
    "\n",
    "def validation_mse(model: Network, x_val: torch.Tensor, y_val: torch.Tensor, device) -> float:\n",
    "    \"\"\"Compute MSE on validation set using the mean head of the model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        mean, _ = model.forward(x_val.to(device))\n",
    "        mse = nn.MSELoss()(mean.to(device), y_val.to(device))\n",
    "    return mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d04f3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_size_sweep(x: torch.Tensor, y: torch.Tensor,\n",
    "                   max_epochs: int = 100,\n",
    "                   n_sizes: int = 20):\n",
    "    \"\"\"\n",
    "    Train networks with different hidden sizes and record validation MSE curves.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hidden_sizes : list[int]\n",
    "    val_mse_curves : np.ndarray of shape (n_sizes, max_epochs)\n",
    "    \"\"\"\n",
    "    # split once so all models see the same train/val split\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x, y, test_size=0.2, shuffle=True\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"using device: {device}\")\n",
    "    \n",
    "    x_train = x_train.to(device)\n",
    "    x_test = x_val.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    y_test = y_val.to(device)\n",
    "\n",
    "    # choose 20 hidden sizes, e.g. from 20 to 400\n",
    "    hidden_sizes = np.linspace(20, 150, n_sizes, dtype=int)\n",
    "\n",
    "    val_mse_curves = np.zeros((n_sizes, max_epochs))\n",
    "    \n",
    "\n",
    "    for i, h_size in enumerate(hidden_sizes):\n",
    "        print(f\"\\n=== Training model with h_size = {h_size} ===\")\n",
    "        arr = [-1,-1,-1]\n",
    "        for j in range(3):\n",
    "            net = Network(h_size=h_size)\n",
    "            net = net.to(device)\n",
    "\n",
    "            net.train((x_train,y_train), epochs=max_epochs, cp=20)\n",
    "\n",
    "                # compute validation MSE\n",
    "            mse_val = validation_mse(net, x_val, y_val, device)\n",
    "            arr[j] = mse_val\n",
    "            \n",
    "        val_mse_curves[i, max_epochs-1] = sum(arr)/3\n",
    "\n",
    "    return hidden_sizes, val_mse_curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ed59497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "\n",
      "=== Training model with h_size = 20 ===\n",
      "iteration 20/5000, loss = 15.163732528686523, 15.124886512756348\n",
      "iteration 40/5000, loss = 13.878435134887695, 13.80697250366211\n",
      "iteration 60/5000, loss = 11.722997665405273, 11.611547470092773\n",
      "iteration 80/5000, loss = 9.709266662597656, 9.652114868164062\n",
      "iteration 100/5000, loss = 8.958100318908691, 8.949578285217285\n",
      "iteration 120/5000, loss = 8.763777732849121, 8.764944076538086\n",
      "iteration 140/5000, loss = 8.567866325378418, 8.569952964782715\n",
      "iteration 160/5000, loss = 8.468700408935547, 8.471020698547363\n",
      "iteration 180/5000, loss = 8.372944831848145, 8.375967979431152\n",
      "iteration 200/5000, loss = 8.283133506774902, 8.28659725189209\n",
      "iteration 220/5000, loss = 8.202117919921875, 8.206003189086914\n",
      "iteration 240/5000, loss = 8.134517669677734, 8.139158248901367\n",
      "iteration 260/5000, loss = 8.080137252807617, 8.085368156433105\n",
      "iteration 280/5000, loss = 8.033408164978027, 8.038864135742188\n",
      "iteration 300/5000, loss = 7.991274356842041, 7.996886253356934\n",
      "iteration 320/5000, loss = 7.956136226654053, 7.962061405181885\n",
      "iteration 340/5000, loss = 7.927684783935547, 7.93386697769165\n",
      "iteration 360/5000, loss = 7.9031901359558105, 7.909441947937012\n",
      "iteration 380/5000, loss = 7.881453037261963, 7.887694358825684\n",
      "iteration 400/5000, loss = 7.861934661865234, 7.86819314956665\n",
      "iteration 420/5000, loss = 7.844293117523193, 7.850614070892334\n",
      "iteration 440/5000, loss = 7.828225135803223, 7.834605693817139\n",
      "iteration 460/5000, loss = 7.813326358795166, 7.819764137268066\n",
      "iteration 480/5000, loss = 7.7993693351745605, 7.805897235870361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hidden_sizes, val_mse_curves \u001b[38;5;241m=\u001b[39m \u001b[43mrun_size_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m final_mse \u001b[38;5;241m=\u001b[39m val_mse_curves[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn [28], line 37\u001b[0m, in \u001b[0;36mrun_size_sweep\u001b[0;34m(x, y, max_epochs, n_sizes)\u001b[0m\n\u001b[1;32m     34\u001b[0m net \u001b[38;5;241m=\u001b[39m Network(h_size\u001b[38;5;241m=\u001b[39mh_size)\n\u001b[1;32m     35\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# compute validation MSE\u001b[39;00m\n\u001b[1;32m     40\u001b[0m mse_val \u001b[38;5;241m=\u001b[39m validation_mse(net, x_val, y_val, device)\n",
      "Cell \u001b[0;32mIn [23], line 68\u001b[0m, in \u001b[0;36mNetwork.train\u001b[0;34m(self, train_data, epochs, cp)\u001b[0m\n\u001b[1;32m     64\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     iteration_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(iteration_loss)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# validate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [23], line 54\u001b[0m, in \u001b[0;36mNetwork.train_epoch\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_sizes, val_mse_curves = run_size_sweep(x, y, max_epochs=5000, n_sizes=1)\n",
    "\n",
    "final_mse = val_mse_curves[:, -1]  # MSE after last epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c513bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(hidden_sizes, final_mse, marker=\"o\")\n",
    "plt.xlabel(\"Hidden layer size (h_size)\")\n",
    "plt.ylabel(\"Validation MSE (final epoch)\")\n",
    "plt.title(\"Final validation MSE vs. model size\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc35c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = Path(\".\")          # current directory = Mopo_test\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "plt.savefig(OUTDIR / \"loss_curve.png\", dpi=200)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12f67423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape = torch.Size([800000, 23])\n",
      "x_test shape = torch.Size([200000, 23])\n",
      "y_train shape = torch.Size([800000, 18])\n",
      "y_test shape = torch.Size([200000, 18])\n",
      "using device: cuda\n",
      "iteration 10/10000, loss = 15.24105453491211, 15.027470588684082\n",
      "iteration 20/10000, loss = 12.67645263671875, 12.392057418823242\n",
      "iteration 30/10000, loss = 10.263128280639648, 10.097777366638184\n",
      "iteration 40/10000, loss = 9.457069396972656, 9.406291961669922\n",
      "iteration 50/10000, loss = 8.951292037963867, 8.916265487670898\n",
      "iteration 60/10000, loss = 8.654077529907227, 8.619485855102539\n",
      "iteration 70/10000, loss = 8.349952697753906, 8.321093559265137\n",
      "iteration 80/10000, loss = 8.068102836608887, 8.037579536437988\n",
      "iteration 90/10000, loss = 7.769552707672119, 7.736114501953125\n",
      "iteration 100/10000, loss = 7.438640594482422, 7.400972843170166\n",
      "iteration 110/10000, loss = 7.095208644866943, 7.060754776000977\n",
      "iteration 120/10000, loss = 6.812288761138916, 6.783164024353027\n",
      "iteration 130/10000, loss = 6.552711009979248, 6.526123523712158\n",
      "iteration 140/10000, loss = 6.326607704162598, 6.304981231689453\n",
      "iteration 150/10000, loss = 6.149737358093262, 6.133969306945801\n",
      "iteration 160/10000, loss = 6.026787757873535, 6.013516902923584\n",
      "iteration 170/10000, loss = 5.9219069480896, 5.909284591674805\n",
      "iteration 180/10000, loss = 5.828315258026123, 5.81580924987793\n",
      "iteration 190/10000, loss = 5.742801666259766, 5.730170726776123\n",
      "iteration 200/10000, loss = 5.663914680480957, 5.651672840118408\n",
      "iteration 210/10000, loss = 5.592334270477295, 5.580387115478516\n",
      "iteration 220/10000, loss = 5.527947902679443, 5.517092227935791\n",
      "iteration 230/10000, loss = 5.467345714569092, 5.456530570983887\n",
      "iteration 240/10000, loss = 5.411723613739014, 5.400986194610596\n",
      "iteration 250/10000, loss = 5.357909202575684, 5.347745418548584\n",
      "iteration 260/10000, loss = 5.306522846221924, 5.296607971191406\n",
      "iteration 270/10000, loss = 5.255564212799072, 5.245041847229004\n",
      "iteration 280/10000, loss = 5.197759628295898, 5.186107635498047\n",
      "iteration 290/10000, loss = 5.130340099334717, 5.119089126586914\n",
      "iteration 300/10000, loss = 5.06288480758667, 5.051938056945801\n",
      "iteration 310/10000, loss = 4.997347831726074, 4.986269950866699\n",
      "iteration 320/10000, loss = 4.928713798522949, 4.916649341583252\n",
      "iteration 330/10000, loss = 4.857951641082764, 4.846834182739258\n",
      "iteration 340/10000, loss = 4.797985553741455, 4.788253307342529\n",
      "iteration 350/10000, loss = 4.745007038116455, 4.736034870147705\n",
      "iteration 360/10000, loss = 4.696031093597412, 4.688055038452148\n",
      "iteration 370/10000, loss = 4.6520795822143555, 4.643448352813721\n",
      "iteration 380/10000, loss = 4.608510971069336, 4.600927352905273\n",
      "iteration 390/10000, loss = 4.567924976348877, 4.560622692108154\n",
      "iteration 400/10000, loss = 4.528647422790527, 4.522169589996338\n",
      "iteration 410/10000, loss = 4.483930587768555, 4.476245403289795\n",
      "iteration 420/10000, loss = 4.4381103515625, 4.432007789611816\n",
      "iteration 430/10000, loss = 4.403611660003662, 4.396933555603027\n",
      "iteration 440/10000, loss = 4.373706817626953, 4.3670220375061035\n",
      "iteration 450/10000, loss = 4.346744537353516, 4.340358257293701\n",
      "iteration 460/10000, loss = 4.3220534324646, 4.315998077392578\n",
      "iteration 470/10000, loss = 4.30090856552124, 4.3007283210754395\n",
      "iteration 480/10000, loss = 4.284524917602539, 4.276599884033203\n",
      "iteration 490/10000, loss = 4.2632575035095215, 4.258449554443359\n",
      "iteration 500/10000, loss = 4.245645046234131, 4.240313529968262\n",
      "iteration 510/10000, loss = 4.228142261505127, 4.22265100479126\n",
      "iteration 520/10000, loss = 4.211452007293701, 4.206015110015869\n",
      "iteration 530/10000, loss = 4.195516586303711, 4.1907057762146\n",
      "iteration 540/10000, loss = 4.1902079582214355, 4.184759616851807\n",
      "iteration 550/10000, loss = 4.168032169342041, 4.160556793212891\n",
      "iteration 560/10000, loss = 4.151468276977539, 4.147082805633545\n",
      "iteration 570/10000, loss = 4.138708591461182, 4.131986618041992\n",
      "iteration 580/10000, loss = 4.120284080505371, 4.113836288452148\n",
      "iteration 590/10000, loss = 4.107518672943115, 4.105499267578125\n",
      "iteration 600/10000, loss = 4.077017307281494, 4.076145648956299\n",
      "iteration 610/10000, loss = 4.060151100158691, 4.0560431480407715\n",
      "iteration 620/10000, loss = 4.046152591705322, 4.041635990142822\n",
      "iteration 630/10000, loss = 4.03209924697876, 4.027608394622803\n",
      "iteration 640/10000, loss = 4.024266719818115, 4.017519950866699\n",
      "iteration 650/10000, loss = 4.011887073516846, 4.004439830780029\n",
      "iteration 660/10000, loss = 3.9957990646362305, 3.9929118156433105\n",
      "iteration 670/10000, loss = 3.9862022399902344, 3.9830360412597656\n",
      "iteration 680/10000, loss = 3.974756956100464, 3.9714927673339844\n",
      "iteration 690/10000, loss = 3.965963840484619, 3.961345911026001\n",
      "iteration 700/10000, loss = 3.9623959064483643, 3.958157777786255\n",
      "iteration 710/10000, loss = 3.9520041942596436, 3.946420192718506\n",
      "iteration 720/10000, loss = 3.9375834465026855, 3.9331166744232178\n",
      "iteration 730/10000, loss = 3.927253484725952, 3.92488956451416\n",
      "iteration 740/10000, loss = 3.9184608459472656, 3.91550874710083\n",
      "iteration 750/10000, loss = 3.91544508934021, 3.911113977432251\n",
      "iteration 760/10000, loss = 3.9094605445861816, 3.9053444862365723\n",
      "iteration 770/10000, loss = 3.9003372192382812, 3.8947041034698486\n",
      "iteration 780/10000, loss = 3.8866617679595947, 3.883913278579712\n",
      "iteration 790/10000, loss = 3.8792827129364014, 3.8785905838012695\n",
      "iteration 800/10000, loss = 3.8714487552642822, 3.8702704906463623\n",
      "iteration 810/10000, loss = 3.863908290863037, 3.861983299255371\n",
      "iteration 820/10000, loss = 3.8596603870391846, 3.855421543121338\n",
      "iteration 830/10000, loss = 3.850440263748169, 3.8508267402648926\n",
      "iteration 840/10000, loss = 3.847177028656006, 3.8463900089263916\n",
      "iteration 850/10000, loss = 3.839540958404541, 3.8380961418151855\n",
      "iteration 860/10000, loss = 3.830993890762329, 3.8298041820526123\n",
      "iteration 870/10000, loss = 3.831101179122925, 3.8267340660095215\n",
      "iteration 880/10000, loss = 3.826394557952881, 3.820131301879883\n",
      "iteration 890/10000, loss = 3.814606189727783, 3.814307451248169\n",
      "iteration 900/10000, loss = 3.7943825721740723, 3.793097972869873\n",
      "iteration 910/10000, loss = 3.776041269302368, 3.7898166179656982\n",
      "iteration 920/10000, loss = 3.741098642349243, 3.7340476512908936\n",
      "iteration 930/10000, loss = 3.7080938816070557, 3.7065789699554443\n",
      "iteration 940/10000, loss = 3.6893231868743896, 3.686349391937256\n",
      "iteration 950/10000, loss = 3.6761229038238525, 3.6731605529785156\n",
      "iteration 960/10000, loss = 3.6635444164276123, 3.661297559738159\n",
      "iteration 970/10000, loss = 3.656236410140991, 3.653790235519409\n",
      "iteration 980/10000, loss = 3.6455538272857666, 3.6439368724823\n",
      "iteration 990/10000, loss = 3.6351211071014404, 3.633378744125366\n",
      "iteration 1000/10000, loss = 3.625727891921997, 3.6237847805023193\n",
      "iteration 1010/10000, loss = 3.6189215183258057, 3.615910530090332\n",
      "iteration 1020/10000, loss = 3.615506887435913, 3.61248517036438\n",
      "iteration 1030/10000, loss = 3.6081032752990723, 3.6060712337493896\n",
      "iteration 1040/10000, loss = 3.59867787361145, 3.5974295139312744\n",
      "iteration 1050/10000, loss = 3.5893638134002686, 3.587355613708496\n",
      "iteration 1060/10000, loss = 3.588099956512451, 3.582489013671875\n",
      "iteration 1070/10000, loss = 3.5776257514953613, 3.5736000537872314\n",
      "iteration 1080/10000, loss = 3.576719045639038, 3.568981885910034\n",
      "iteration 1090/10000, loss = 3.552182674407959, 3.5466978549957275\n",
      "iteration 1100/10000, loss = 3.5257930755615234, 3.524702787399292\n",
      "iteration 1110/10000, loss = 3.5072007179260254, 3.502662181854248\n",
      "iteration 1120/10000, loss = 3.490309715270996, 3.4881653785705566\n",
      "iteration 1130/10000, loss = 3.4806642532348633, 3.4765303134918213\n",
      "iteration 1140/10000, loss = 3.4695639610290527, 3.46699595451355\n",
      "iteration 1150/10000, loss = 3.4647932052612305, 3.4678285121917725\n",
      "iteration 1160/10000, loss = 3.4623451232910156, 3.458547353744507\n",
      "iteration 1170/10000, loss = 3.4461333751678467, 3.4438652992248535\n",
      "iteration 1180/10000, loss = 3.4396841526031494, 3.436990261077881\n",
      "iteration 1190/10000, loss = 3.432177782058716, 3.428839921951294\n",
      "iteration 1200/10000, loss = 3.4299559593200684, 3.4308178424835205\n",
      "iteration 1210/10000, loss = 3.426119565963745, 3.4166319370269775\n",
      "iteration 1220/10000, loss = 3.4135890007019043, 3.409616708755493\n",
      "iteration 1230/10000, loss = 3.409759044647217, 3.4064950942993164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1240/10000, loss = 3.402620792388916, 3.399160861968994\n",
      "iteration 1250/10000, loss = 3.395944595336914, 3.391622304916382\n",
      "iteration 1260/10000, loss = 3.39245343208313, 3.392141103744507\n",
      "iteration 1270/10000, loss = 3.3952674865722656, 3.380378484725952\n",
      "iteration 1280/10000, loss = 3.380678176879883, 3.3726770877838135\n",
      "iteration 1290/10000, loss = 3.368889570236206, 3.3661961555480957\n",
      "iteration 1300/10000, loss = 3.3791661262512207, 3.39695143699646\n",
      "iteration 1310/10000, loss = 3.3666131496429443, 3.3710381984710693\n",
      "iteration 1320/10000, loss = 3.353760004043579, 3.3513145446777344\n",
      "iteration 1330/10000, loss = 3.347468376159668, 3.3418772220611572\n",
      "iteration 1340/10000, loss = 3.347917079925537, 3.3507721424102783\n",
      "iteration 1350/10000, loss = 3.342885732650757, 3.348477363586426\n",
      "iteration 1360/10000, loss = 3.3311729431152344, 3.334256172180176\n",
      "iteration 1370/10000, loss = 3.3300819396972656, 3.324204921722412\n",
      "iteration 1380/10000, loss = 3.3217601776123047, 3.318300485610962\n",
      "iteration 1390/10000, loss = 3.3166229724884033, 3.3129167556762695\n",
      "iteration 1400/10000, loss = 3.311370849609375, 3.308058977127075\n",
      "iteration 1410/10000, loss = 3.3145625591278076, 3.307178258895874\n",
      "iteration 1420/10000, loss = 3.303187608718872, 3.30098819732666\n",
      "iteration 1430/10000, loss = 3.3039379119873047, 3.295853614807129\n",
      "iteration 1440/10000, loss = 3.346851110458374, 3.340848684310913\n",
      "iteration 1450/10000, loss = 3.3087832927703857, 3.299126148223877\n",
      "iteration 1460/10000, loss = 3.2995693683624268, 3.2859091758728027\n",
      "iteration 1470/10000, loss = 3.2839951515197754, 3.2789549827575684\n",
      "iteration 1480/10000, loss = 3.2776503562927246, 3.27380108833313\n",
      "iteration 1490/10000, loss = 3.284435510635376, 3.2866628170013428\n",
      "iteration 1500/10000, loss = 3.2790215015411377, 3.2721567153930664\n",
      "iteration 1510/10000, loss = 3.2704079151153564, 3.26330828666687\n",
      "iteration 1520/10000, loss = 3.266016721725464, 3.26255464553833\n",
      "iteration 1530/10000, loss = 3.2613327503204346, 3.2570855617523193\n",
      "iteration 1540/10000, loss = 3.275930643081665, 3.281991958618164\n",
      "iteration 1550/10000, loss = 3.2592244148254395, 3.261275053024292\n",
      "iteration 1560/10000, loss = 3.2527055740356445, 3.251965284347534\n",
      "iteration 1570/10000, loss = 3.2661354541778564, 3.2988266944885254\n",
      "iteration 1580/10000, loss = 3.267592668533325, 3.2450056076049805\n",
      "iteration 1590/10000, loss = 3.2507636547088623, 3.238237142562866\n",
      "iteration 1600/10000, loss = 3.239182710647583, 3.2347493171691895\n",
      "iteration 1610/10000, loss = 3.2363500595092773, 3.2302253246307373\n",
      "iteration 1620/10000, loss = 3.233344793319702, 3.2288382053375244\n",
      "iteration 1630/10000, loss = 3.229438304901123, 3.2286839485168457\n",
      "iteration 1640/10000, loss = 3.2254562377929688, 3.220611095428467\n",
      "iteration 1650/10000, loss = 3.224391460418701, 3.220984697341919\n",
      "iteration 1660/10000, loss = 3.227363348007202, 3.2240536212921143\n",
      "iteration 1670/10000, loss = 3.2208383083343506, 3.212742328643799\n",
      "iteration 1680/10000, loss = 3.214102029800415, 3.208189010620117\n",
      "iteration 1690/10000, loss = 3.2241017818450928, 3.2172889709472656\n",
      "iteration 1700/10000, loss = 3.2092063426971436, 3.20613694190979\n",
      "iteration 1710/10000, loss = 3.2074086666107178, 3.20450496673584\n",
      "iteration 1720/10000, loss = 3.2215542793273926, 3.2145326137542725\n",
      "iteration 1730/10000, loss = 3.204563617706299, 3.1954333782196045\n",
      "iteration 1740/10000, loss = 3.1998534202575684, 3.1968228816986084\n",
      "iteration 1750/10000, loss = 3.1995108127593994, 3.1955785751342773\n",
      "iteration 1760/10000, loss = 3.195970058441162, 3.1915667057037354\n",
      "iteration 1770/10000, loss = 3.2038896083831787, 3.2092020511627197\n",
      "iteration 1780/10000, loss = 3.20124888420105, 3.19193959236145\n",
      "iteration 1790/10000, loss = 3.191257953643799, 3.1833720207214355\n",
      "iteration 1800/10000, loss = 3.1848957538604736, 3.179293155670166\n",
      "iteration 1810/10000, loss = 3.180302858352661, 3.175201416015625\n",
      "iteration 1820/10000, loss = 3.18172025680542, 3.1793227195739746\n",
      "iteration 1830/10000, loss = 3.180145263671875, 3.174694538116455\n",
      "iteration 1840/10000, loss = 3.1833841800689697, 3.2021450996398926\n",
      "iteration 1850/10000, loss = 3.1770763397216797, 3.1826364994049072\n",
      "iteration 1860/10000, loss = 3.176499366760254, 3.164314031600952\n",
      "iteration 1870/10000, loss = 3.166034460067749, 3.1588077545166016\n",
      "iteration 1880/10000, loss = 3.1639389991760254, 3.158658981323242\n",
      "iteration 1890/10000, loss = 3.1590957641601562, 3.1531362533569336\n",
      "iteration 1900/10000, loss = 3.1644487380981445, 3.163827419281006\n",
      "iteration 1910/10000, loss = 3.1657555103302, 3.1506149768829346\n",
      "iteration 1920/10000, loss = 3.1618223190307617, 3.153263568878174\n",
      "iteration 1930/10000, loss = 3.1510283946990967, 3.1440556049346924\n",
      "iteration 1940/10000, loss = 3.1819076538085938, 3.1691267490386963\n",
      "iteration 1950/10000, loss = 3.1514201164245605, 3.141098737716675\n",
      "iteration 1960/10000, loss = 3.1510426998138428, 3.1450228691101074\n",
      "iteration 1970/10000, loss = 3.144704580307007, 3.1405375003814697\n",
      "iteration 1980/10000, loss = 3.139298915863037, 3.1333541870117188\n",
      "iteration 1990/10000, loss = 3.1745622158050537, 3.179661512374878\n",
      "iteration 2000/10000, loss = 3.1397340297698975, 3.1355113983154297\n",
      "iteration 2010/10000, loss = 3.1379168033599854, 3.133422374725342\n",
      "iteration 2020/10000, loss = 3.1322128772735596, 3.1270456314086914\n",
      "iteration 2030/10000, loss = 3.1396195888519287, 3.126002073287964\n",
      "iteration 2040/10000, loss = 3.1349868774414062, 3.1247975826263428\n",
      "iteration 2050/10000, loss = 3.133178949356079, 3.1324381828308105\n",
      "iteration 2060/10000, loss = 3.125786781311035, 3.122170925140381\n",
      "iteration 2070/10000, loss = 3.1241791248321533, 3.1226372718811035\n",
      "iteration 2080/10000, loss = 3.126842498779297, 3.116713523864746\n",
      "iteration 2090/10000, loss = 3.118030548095703, 3.1111388206481934\n",
      "iteration 2100/10000, loss = 3.142409324645996, 3.1403236389160156\n",
      "iteration 2110/10000, loss = 3.126955509185791, 3.1186201572418213\n",
      "iteration 2120/10000, loss = 3.1235389709472656, 3.118102550506592\n",
      "iteration 2130/10000, loss = 3.113396644592285, 3.111154794692993\n",
      "iteration 2140/10000, loss = 3.1138417720794678, 3.1066277027130127\n",
      "iteration 2150/10000, loss = 3.1104736328125, 3.1065688133239746\n",
      "iteration 2160/10000, loss = 3.1232948303222656, 3.1526243686676025\n",
      "iteration 2170/10000, loss = 3.11318302154541, 3.1247687339782715\n",
      "iteration 2180/10000, loss = 3.112886905670166, 3.100505828857422\n",
      "iteration 2190/10000, loss = 3.104811191558838, 3.0964927673339844\n",
      "iteration 2200/10000, loss = 3.1008381843566895, 3.0939066410064697\n",
      "iteration 2210/10000, loss = 3.1099183559417725, 3.0983493328094482\n",
      "iteration 2220/10000, loss = 3.098024606704712, 3.089820146560669\n",
      "iteration 2230/10000, loss = 3.1070938110351562, 3.1046805381774902\n",
      "iteration 2240/10000, loss = 3.096907615661621, 3.08561110496521\n",
      "iteration 2250/10000, loss = 3.0943546295166016, 3.0897886753082275\n",
      "iteration 2260/10000, loss = 3.094974994659424, 3.1067311763763428\n",
      "iteration 2270/10000, loss = 3.090772867202759, 3.0934348106384277\n",
      "iteration 2280/10000, loss = 3.0883522033691406, 3.0797929763793945\n",
      "iteration 2290/10000, loss = 3.0894615650177, 3.0838117599487305\n",
      "iteration 2300/10000, loss = 3.082789659500122, 3.0769450664520264\n",
      "iteration 2310/10000, loss = 3.0927014350891113, 3.078458070755005\n",
      "iteration 2320/10000, loss = 3.0775368213653564, 3.0705642700195312\n",
      "iteration 2330/10000, loss = 3.087228536605835, 3.0759334564208984\n",
      "iteration 2340/10000, loss = 3.074946880340576, 3.0676004886627197\n",
      "iteration 2350/10000, loss = 3.074287176132202, 3.067295789718628\n",
      "iteration 2360/10000, loss = 3.118919849395752, 3.097127914428711\n",
      "iteration 2370/10000, loss = 3.073859691619873, 3.0665295124053955\n",
      "iteration 2380/10000, loss = 3.067920207977295, 3.06135630607605\n",
      "iteration 2390/10000, loss = 3.068711042404175, 3.0628132820129395\n",
      "iteration 2400/10000, loss = 3.063636064529419, 3.0580875873565674\n",
      "iteration 2410/10000, loss = 3.0794692039489746, 3.088052749633789\n",
      "iteration 2420/10000, loss = 3.064915418624878, 3.055690288543701\n",
      "iteration 2430/10000, loss = 3.070648670196533, 3.063992977142334\n",
      "iteration 2440/10000, loss = 3.0610902309417725, 3.0535836219787598\n",
      "iteration 2450/10000, loss = 3.0689237117767334, 3.060476779937744\n",
      "iteration 2460/10000, loss = 3.05666184425354, 3.048408269882202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2470/10000, loss = 3.084298610687256, 3.0734264850616455\n",
      "iteration 2480/10000, loss = 3.04911208152771, 3.0456550121307373\n",
      "iteration 2490/10000, loss = 3.047267436981201, 3.042048692703247\n",
      "iteration 2500/10000, loss = 3.056880474090576, 3.0420684814453125\n",
      "iteration 2510/10000, loss = 3.043489694595337, 3.0378851890563965\n",
      "iteration 2520/10000, loss = 3.0536742210388184, 3.0377376079559326\n",
      "iteration 2530/10000, loss = 3.068567991256714, 3.0540549755096436\n",
      "iteration 2540/10000, loss = 3.0473673343658447, 3.0397439002990723\n",
      "iteration 2550/10000, loss = 3.062544584274292, 3.063999652862549\n",
      "iteration 2560/10000, loss = 3.053281307220459, 3.0381457805633545\n",
      "iteration 2570/10000, loss = 3.0367493629455566, 3.03078556060791\n",
      "iteration 2580/10000, loss = 3.0484659671783447, 3.0399434566497803\n",
      "iteration 2590/10000, loss = 3.033933401107788, 3.0237624645233154\n",
      "iteration 2600/10000, loss = 3.051676034927368, 3.0508480072021484\n",
      "iteration 2610/10000, loss = 3.0345041751861572, 3.0211009979248047\n",
      "iteration 2620/10000, loss = 3.026204824447632, 3.022300958633423\n",
      "iteration 2630/10000, loss = 3.039085865020752, 3.0599160194396973\n",
      "iteration 2640/10000, loss = 3.028752088546753, 3.026124954223633\n",
      "iteration 2650/10000, loss = 3.0197482109069824, 3.016568899154663\n",
      "iteration 2660/10000, loss = 3.021615982055664, 3.015099287033081\n",
      "iteration 2670/10000, loss = 3.0227367877960205, 3.0222785472869873\n",
      "iteration 2680/10000, loss = 3.024885416030884, 3.021477222442627\n",
      "iteration 2690/10000, loss = 3.0223820209503174, 3.0165631771087646\n",
      "iteration 2700/10000, loss = 3.012916326522827, 3.0099167823791504\n",
      "iteration 2710/10000, loss = 3.0108180046081543, 3.010526180267334\n",
      "iteration 2720/10000, loss = 3.010812282562256, 3.00974440574646\n",
      "iteration 2730/10000, loss = 3.0133728981018066, 3.01804518699646\n",
      "iteration 2740/10000, loss = 3.016005516052246, 3.0010218620300293\n",
      "iteration 2750/10000, loss = 3.0130457878112793, 3.009432077407837\n",
      "iteration 2760/10000, loss = 3.0145909786224365, 2.997278928756714\n",
      "iteration 2770/10000, loss = 2.998354434967041, 2.995175838470459\n",
      "iteration 2780/10000, loss = 3.0400938987731934, 3.0085694789886475\n",
      "iteration 2790/10000, loss = 2.9991090297698975, 2.9982059001922607\n",
      "iteration 2800/10000, loss = 2.996084690093994, 2.993048906326294\n",
      "iteration 2810/10000, loss = 3.01188325881958, 3.0074386596679688\n",
      "iteration 2820/10000, loss = 2.9996471405029297, 2.989856243133545\n",
      "iteration 2830/10000, loss = 2.996290683746338, 2.9973626136779785\n",
      "iteration 2840/10000, loss = 2.99053692817688, 2.983306646347046\n",
      "iteration 2850/10000, loss = 2.985179901123047, 2.9786980152130127\n",
      "iteration 2860/10000, loss = 3.045961856842041, 3.038060188293457\n",
      "iteration 2870/10000, loss = 2.9868764877319336, 2.9846975803375244\n",
      "iteration 2880/10000, loss = 2.9955215454101562, 3.0049867630004883\n",
      "iteration 2890/10000, loss = 3.0306665897369385, 3.0034372806549072\n",
      "iteration 2900/10000, loss = 2.988081932067871, 2.9903030395507812\n",
      "iteration 2910/10000, loss = 2.9841465950012207, 2.977294445037842\n",
      "iteration 2920/10000, loss = 2.9805312156677246, 2.973004102706909\n",
      "iteration 2930/10000, loss = 2.9877867698669434, 2.9774980545043945\n",
      "iteration 2940/10000, loss = 2.9741687774658203, 2.9658801555633545\n",
      "iteration 2950/10000, loss = 2.9977340698242188, 3.0007271766662598\n",
      "iteration 2960/10000, loss = 2.9802443981170654, 2.9838669300079346\n",
      "iteration 2970/10000, loss = 2.9789910316467285, 2.9780027866363525\n",
      "iteration 2980/10000, loss = 2.970794916152954, 2.9636728763580322\n",
      "iteration 2990/10000, loss = 2.9659104347229004, 2.959303617477417\n",
      "iteration 3000/10000, loss = 3.0072624683380127, 3.000323534011841\n",
      "iteration 3010/10000, loss = 2.977620840072632, 2.966909170150757\n",
      "iteration 3020/10000, loss = 2.9868364334106445, 2.9713327884674072\n",
      "iteration 3030/10000, loss = 2.9614875316619873, 2.958991765975952\n",
      "iteration 3040/10000, loss = 2.9646830558776855, 2.9552581310272217\n",
      "iteration 3050/10000, loss = 2.9636831283569336, 2.959763288497925\n",
      "iteration 3060/10000, loss = 2.96966814994812, 2.9729361534118652\n",
      "iteration 3070/10000, loss = 2.9648597240448, 2.9613990783691406\n",
      "iteration 3080/10000, loss = 2.9935214519500732, 2.999983310699463\n",
      "iteration 3090/10000, loss = 2.966771125793457, 2.9551501274108887\n",
      "iteration 3100/10000, loss = 2.956387519836426, 2.9476869106292725\n",
      "iteration 3110/10000, loss = 2.9494242668151855, 2.943481922149658\n",
      "iteration 3120/10000, loss = 3.0270299911499023, 3.0205376148223877\n",
      "iteration 3130/10000, loss = 2.9605653285980225, 2.9696438312530518\n",
      "iteration 3140/10000, loss = 2.9474785327911377, 2.9452977180480957\n",
      "iteration 3150/10000, loss = 2.961021661758423, 2.9689767360687256\n",
      "iteration 3160/10000, loss = 2.9735355377197266, 2.951215982437134\n",
      "iteration 3170/10000, loss = 2.9530930519104004, 2.944528579711914\n",
      "iteration 3180/10000, loss = 2.9522550106048584, 2.9470722675323486\n",
      "iteration 3190/10000, loss = 2.9416861534118652, 2.9381773471832275\n",
      "iteration 3200/10000, loss = 2.9448001384735107, 2.937375068664551\n",
      "iteration 3210/10000, loss = 2.936871290206909, 2.930967330932617\n",
      "iteration 3220/10000, loss = 3.12707781791687, 3.152871608734131\n",
      "iteration 3230/10000, loss = 2.9930319786071777, 2.977152109146118\n",
      "iteration 3240/10000, loss = 2.965388536453247, 2.944655656814575\n",
      "iteration 3250/10000, loss = 2.939366102218628, 2.9324541091918945\n",
      "iteration 3260/10000, loss = 2.935619592666626, 2.9283909797668457\n",
      "iteration 3270/10000, loss = 2.9353368282318115, 2.9322900772094727\n",
      "iteration 3280/10000, loss = 2.9518673419952393, 2.9432506561279297\n",
      "iteration 3290/10000, loss = 2.9305906295776367, 2.931234121322632\n",
      "iteration 3300/10000, loss = 2.9308674335479736, 2.923013210296631\n",
      "iteration 3310/10000, loss = 2.927762508392334, 2.923084020614624\n",
      "iteration 3320/10000, loss = 2.9661221504211426, 2.9931812286376953\n",
      "iteration 3330/10000, loss = 2.9387576580047607, 2.9375979900360107\n",
      "iteration 3340/10000, loss = 2.930396318435669, 2.9254419803619385\n",
      "iteration 3350/10000, loss = 2.9258813858032227, 2.9198832511901855\n",
      "iteration 3360/10000, loss = 2.926682233810425, 2.9217965602874756\n",
      "iteration 3370/10000, loss = 2.9321937561035156, 2.9188742637634277\n",
      "iteration 3380/10000, loss = 2.9213764667510986, 2.914099931716919\n",
      "iteration 3390/10000, loss = 2.9741108417510986, 3.002612829208374\n",
      "iteration 3400/10000, loss = 2.9228627681732178, 2.933581590652466\n",
      "iteration 3410/10000, loss = 2.9215056896209717, 2.915127754211426\n",
      "iteration 3420/10000, loss = 2.921567440032959, 2.917227029800415\n",
      "iteration 3430/10000, loss = 2.917426109313965, 2.91135311126709\n",
      "iteration 3440/10000, loss = 2.9269208908081055, 2.9122586250305176\n",
      "iteration 3450/10000, loss = 2.9183719158172607, 2.9075469970703125\n",
      "iteration 3460/10000, loss = 2.9308416843414307, 2.926596164703369\n",
      "iteration 3470/10000, loss = 2.912468194961548, 2.9047300815582275\n",
      "iteration 3480/10000, loss = 2.984473705291748, 2.952409029006958\n",
      "iteration 3490/10000, loss = 2.915066957473755, 2.9201583862304688\n",
      "iteration 3500/10000, loss = 2.929630994796753, 2.9303746223449707\n",
      "iteration 3510/10000, loss = 2.915848970413208, 2.9181559085845947\n",
      "iteration 3520/10000, loss = 2.913876533508301, 2.9104907512664795\n",
      "iteration 3530/10000, loss = 2.91015625, 2.901181697845459\n",
      "iteration 3540/10000, loss = 2.9163944721221924, 2.9163691997528076\n",
      "iteration 3550/10000, loss = 2.9188671112060547, 2.9077508449554443\n",
      "iteration 3560/10000, loss = 2.9113969802856445, 2.898298740386963\n",
      "iteration 3570/10000, loss = 2.91984486579895, 2.9344482421875\n",
      "iteration 3580/10000, loss = 2.924370765686035, 2.9217638969421387\n",
      "iteration 3590/10000, loss = 2.9174633026123047, 2.9026174545288086\n",
      "iteration 3600/10000, loss = 2.911842107772827, 2.9121999740600586\n",
      "iteration 3610/10000, loss = 2.907416582107544, 2.9000282287597656\n",
      "iteration 3620/10000, loss = 2.898338794708252, 2.8906211853027344\n",
      "iteration 3630/10000, loss = 2.91159987449646, 2.918330669403076\n",
      "iteration 3640/10000, loss = 2.903954267501831, 2.8983027935028076\n",
      "iteration 3650/10000, loss = 2.903538942337036, 2.8981800079345703\n",
      "iteration 3660/10000, loss = 2.9003591537475586, 2.894702911376953\n",
      "iteration 3670/10000, loss = 2.9030544757843018, 2.899571657180786\n",
      "iteration 3680/10000, loss = 2.8964037895202637, 2.894618511199951\n",
      "iteration 3690/10000, loss = 2.914980888366699, 2.9037206172943115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3700/10000, loss = 2.894322156906128, 2.892582416534424\n",
      "iteration 3710/10000, loss = 2.9006197452545166, 2.8936092853546143\n",
      "iteration 3720/10000, loss = 2.900926351547241, 2.9022445678710938\n",
      "iteration 3730/10000, loss = 2.898165464401245, 2.8849263191223145\n",
      "iteration 3740/10000, loss = 2.9016671180725098, 2.9129858016967773\n",
      "iteration 3750/10000, loss = 2.913806200027466, 2.8934459686279297\n",
      "iteration 3760/10000, loss = 2.929326057434082, 2.927421808242798\n",
      "iteration 3770/10000, loss = 2.8937811851501465, 2.886770486831665\n",
      "iteration 3780/10000, loss = 2.884408712387085, 2.8755316734313965\n",
      "iteration 3790/10000, loss = 2.9246528148651123, 2.9279625415802\n",
      "iteration 3800/10000, loss = 2.893381357192993, 2.8773000240325928\n",
      "iteration 3810/10000, loss = 2.88778018951416, 2.8809561729431152\n",
      "iteration 3820/10000, loss = 2.9194700717926025, 2.89646053314209\n",
      "iteration 3830/10000, loss = 2.8952083587646484, 2.8820555210113525\n",
      "iteration 3840/10000, loss = 2.8839612007141113, 2.8838322162628174\n",
      "iteration 3850/10000, loss = 2.8955516815185547, 2.878202438354492\n",
      "iteration 3860/10000, loss = 2.882091760635376, 2.87353777885437\n",
      "iteration 3870/10000, loss = 2.8900742530822754, 2.884495496749878\n",
      "iteration 3880/10000, loss = 2.882716178894043, 2.8777263164520264\n",
      "iteration 3890/10000, loss = 2.872859001159668, 2.8698642253875732\n",
      "iteration 3900/10000, loss = 2.912031888961792, 2.9030911922454834\n",
      "iteration 3910/10000, loss = 2.8925836086273193, 2.875990152359009\n",
      "iteration 3920/10000, loss = 2.880429267883301, 2.872948169708252\n",
      "iteration 3930/10000, loss = 2.8737850189208984, 2.8663840293884277\n",
      "iteration 3940/10000, loss = 2.8699285984039307, 2.8654062747955322\n",
      "iteration 3950/10000, loss = 2.882039785385132, 2.884536027908325\n",
      "iteration 3960/10000, loss = 2.8675599098205566, 2.8636372089385986\n",
      "iteration 3970/10000, loss = 2.8730082511901855, 2.8698673248291016\n",
      "iteration 3980/10000, loss = 2.8921828269958496, 2.8692049980163574\n",
      "iteration 3990/10000, loss = 2.8799660205841064, 2.877455472946167\n",
      "iteration 4000/10000, loss = 2.8645310401916504, 2.861962080001831\n",
      "iteration 4010/10000, loss = 2.8752520084381104, 2.863980531692505\n",
      "iteration 4020/10000, loss = 2.871889591217041, 2.871835947036743\n",
      "iteration 4030/10000, loss = 2.8610196113586426, 2.857924222946167\n",
      "iteration 4040/10000, loss = 2.8996706008911133, 2.902818441390991\n",
      "iteration 4050/10000, loss = 2.871900796890259, 2.8611416816711426\n",
      "iteration 4060/10000, loss = 2.866163492202759, 2.8660707473754883\n",
      "iteration 4070/10000, loss = 2.9225003719329834, 2.9424521923065186\n",
      "iteration 4080/10000, loss = 2.87845516204834, 2.8844285011291504\n",
      "iteration 4090/10000, loss = 2.8656115531921387, 2.8611011505126953\n",
      "iteration 4100/10000, loss = 2.871915340423584, 2.8639509677886963\n",
      "iteration 4110/10000, loss = 2.857788324356079, 2.8490030765533447\n",
      "iteration 4120/10000, loss = 2.869786024093628, 2.87432599067688\n",
      "iteration 4130/10000, loss = 2.854956865310669, 2.8532166481018066\n",
      "iteration 4140/10000, loss = 2.8545479774475098, 2.8461110591888428\n",
      "iteration 4150/10000, loss = 2.8663148880004883, 2.8722167015075684\n",
      "iteration 4160/10000, loss = 2.856818675994873, 2.864419937133789\n",
      "iteration 4170/10000, loss = 2.874861478805542, 2.8691816329956055\n",
      "iteration 4180/10000, loss = 2.861605167388916, 2.846403121948242\n",
      "iteration 4190/10000, loss = 2.8470025062561035, 2.8446261882781982\n",
      "iteration 4200/10000, loss = 2.8568265438079834, 2.8458242416381836\n",
      "iteration 4210/10000, loss = 2.8705313205718994, 2.869525671005249\n",
      "iteration 4220/10000, loss = 2.8559365272521973, 2.847102403640747\n",
      "iteration 4230/10000, loss = 2.843376636505127, 2.8399808406829834\n",
      "iteration 4240/10000, loss = 2.848696231842041, 2.837766170501709\n",
      "iteration 4250/10000, loss = 2.844656229019165, 2.843801975250244\n",
      "iteration 4260/10000, loss = 2.903127908706665, 2.8712775707244873\n",
      "iteration 4270/10000, loss = 2.8609466552734375, 2.847858190536499\n",
      "iteration 4280/10000, loss = 2.846792459487915, 2.8428876399993896\n",
      "iteration 4290/10000, loss = 2.8844046592712402, 2.840013265609741\n",
      "iteration 4300/10000, loss = 2.848129987716675, 2.837035894393921\n",
      "iteration 4310/10000, loss = 2.843902587890625, 2.8422017097473145\n",
      "iteration 4320/10000, loss = 2.8458406925201416, 2.8459253311157227\n",
      "iteration 4330/10000, loss = 2.8560564517974854, 2.8366589546203613\n",
      "iteration 4340/10000, loss = 2.8396973609924316, 2.8337836265563965\n",
      "iteration 4350/10000, loss = 2.844905138015747, 2.833465337753296\n",
      "iteration 4360/10000, loss = 2.8454136848449707, 2.8429691791534424\n",
      "iteration 4370/10000, loss = 2.847486734390259, 2.851712226867676\n",
      "iteration 4380/10000, loss = 2.846824884414673, 2.8312032222747803\n",
      "iteration 4390/10000, loss = 2.846442699432373, 2.844303846359253\n",
      "iteration 4400/10000, loss = 2.858196496963501, 2.842348337173462\n",
      "iteration 4410/10000, loss = 2.8361916542053223, 2.825000047683716\n",
      "iteration 4420/10000, loss = 2.892563819885254, 2.9343080520629883\n",
      "iteration 4430/10000, loss = 2.8356313705444336, 2.853947639465332\n",
      "iteration 4440/10000, loss = 2.8359551429748535, 2.837468147277832\n",
      "iteration 4450/10000, loss = 2.844913959503174, 2.8369081020355225\n",
      "iteration 4460/10000, loss = 2.834721326828003, 2.830714702606201\n",
      "iteration 4470/10000, loss = 2.825373411178589, 2.8188982009887695\n",
      "iteration 4480/10000, loss = 2.930036783218384, 2.9128923416137695\n",
      "iteration 4490/10000, loss = 2.841827392578125, 2.854374647140503\n",
      "iteration 4500/10000, loss = 2.8257827758789062, 2.827176570892334\n",
      "iteration 4510/10000, loss = 2.8362317085266113, 2.829542875289917\n",
      "iteration 4520/10000, loss = 2.8284521102905273, 2.820451498031616\n",
      "iteration 4530/10000, loss = 2.8342626094818115, 2.832765817642212\n",
      "iteration 4540/10000, loss = 2.8520348072052, 2.8235695362091064\n",
      "iteration 4550/10000, loss = 2.834351062774658, 2.8174610137939453\n",
      "iteration 4560/10000, loss = 2.823631525039673, 2.8224244117736816\n",
      "iteration 4570/10000, loss = 2.8260111808776855, 2.816340208053589\n",
      "iteration 4580/10000, loss = 2.825064182281494, 2.816648006439209\n",
      "iteration 4590/10000, loss = 2.8472445011138916, 2.8571245670318604\n",
      "iteration 4600/10000, loss = 2.820584297180176, 2.8241732120513916\n",
      "iteration 4610/10000, loss = 2.827331304550171, 2.8202030658721924\n",
      "iteration 4620/10000, loss = 2.8494744300842285, 2.8650200366973877\n",
      "iteration 4630/10000, loss = 2.8334243297576904, 2.8255362510681152\n",
      "iteration 4640/10000, loss = 2.825317621231079, 2.813826322555542\n",
      "iteration 4650/10000, loss = 2.8178107738494873, 2.8151731491088867\n",
      "iteration 4660/10000, loss = 2.8124430179595947, 2.810138463973999\n",
      "iteration 4670/10000, loss = 2.8547868728637695, 2.8526577949523926\n",
      "iteration 4680/10000, loss = 2.8183517456054688, 2.8194680213928223\n",
      "iteration 4690/10000, loss = 2.859875440597534, 2.8225319385528564\n",
      "iteration 4700/10000, loss = 2.824315071105957, 2.826977491378784\n",
      "iteration 4710/10000, loss = 2.8191893100738525, 2.816147804260254\n",
      "iteration 4720/10000, loss = 2.8244993686676025, 2.8324146270751953\n",
      "iteration 4730/10000, loss = 2.8196353912353516, 2.805593252182007\n",
      "iteration 4740/10000, loss = 2.8149611949920654, 2.8136868476867676\n",
      "iteration 4750/10000, loss = 2.8072102069854736, 2.8045449256896973\n",
      "iteration 4760/10000, loss = 2.8075404167175293, 2.806518793106079\n",
      "iteration 4770/10000, loss = 2.811326503753662, 2.8211114406585693\n",
      "iteration 4780/10000, loss = 2.8125152587890625, 2.8146700859069824\n",
      "iteration 4790/10000, loss = 2.812429189682007, 2.8027002811431885\n",
      "iteration 4800/10000, loss = 2.8305492401123047, 2.8345906734466553\n",
      "iteration 4810/10000, loss = 2.8147096633911133, 2.8016631603240967\n",
      "iteration 4820/10000, loss = 2.808689832687378, 2.8009181022644043\n",
      "iteration 4830/10000, loss = 2.813887596130371, 2.8427820205688477\n",
      "iteration 4840/10000, loss = 2.8076725006103516, 2.8177881240844727\n",
      "iteration 4850/10000, loss = 2.822082042694092, 2.8160479068756104\n",
      "iteration 4860/10000, loss = 2.8126869201660156, 2.7940382957458496\n",
      "iteration 4870/10000, loss = 2.803985357284546, 2.7932496070861816\n",
      "iteration 4880/10000, loss = 2.8055801391601562, 2.8097984790802\n",
      "iteration 4890/10000, loss = 2.798396110534668, 2.80464768409729\n",
      "iteration 4900/10000, loss = 2.816835880279541, 2.8090245723724365\n",
      "iteration 4910/10000, loss = 2.8225739002227783, 2.799581527709961\n",
      "iteration 4920/10000, loss = 2.794753074645996, 2.7887375354766846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4930/10000, loss = 2.8135740756988525, 2.8164639472961426\n",
      "iteration 4940/10000, loss = 2.7993807792663574, 2.7880163192749023\n",
      "iteration 4950/10000, loss = 2.7910537719726562, 2.7888553142547607\n",
      "iteration 4960/10000, loss = 2.8509469032287598, 2.8539767265319824\n",
      "iteration 4970/10000, loss = 2.814251184463501, 2.7973804473876953\n",
      "iteration 4980/10000, loss = 2.7970104217529297, 2.790968179702759\n",
      "iteration 4990/10000, loss = 2.8003392219543457, 2.7953643798828125\n",
      "iteration 5000/10000, loss = 2.7866992950439453, 2.786165237426758\n",
      "iteration 5010/10000, loss = 2.806575298309326, 2.809156656265259\n",
      "iteration 5020/10000, loss = 2.8226723670959473, 2.7860705852508545\n",
      "iteration 5030/10000, loss = 2.7865118980407715, 2.7844865322113037\n",
      "iteration 5040/10000, loss = 2.793508291244507, 2.785229444503784\n",
      "iteration 5050/10000, loss = 2.8096652030944824, 2.8308372497558594\n",
      "iteration 5060/10000, loss = 2.795231342315674, 2.7991573810577393\n",
      "iteration 5070/10000, loss = 2.786787271499634, 2.7761902809143066\n",
      "iteration 5080/10000, loss = 2.852294445037842, 2.8650336265563965\n",
      "iteration 5090/10000, loss = 2.7875192165374756, 2.7838945388793945\n",
      "iteration 5100/10000, loss = 2.7875452041625977, 2.7877683639526367\n",
      "iteration 5110/10000, loss = 2.7826318740844727, 2.7767820358276367\n",
      "iteration 5120/10000, loss = 2.822338581085205, 2.8136584758758545\n",
      "iteration 5130/10000, loss = 2.7829322814941406, 2.781958818435669\n",
      "iteration 5140/10000, loss = 2.8120436668395996, 2.822204828262329\n",
      "iteration 5150/10000, loss = 2.8093841075897217, 2.7917678356170654\n",
      "iteration 5160/10000, loss = 2.789292335510254, 2.7748219966888428\n",
      "iteration 5170/10000, loss = 2.777153968811035, 2.7754316329956055\n",
      "iteration 5180/10000, loss = 2.8320858478546143, 2.788316249847412\n",
      "iteration 5190/10000, loss = 2.787480354309082, 2.7947628498077393\n",
      "iteration 5200/10000, loss = 2.7748584747314453, 2.771395444869995\n",
      "iteration 5210/10000, loss = 2.799731731414795, 2.8176214694976807\n",
      "iteration 5220/10000, loss = 2.800807237625122, 2.799726963043213\n",
      "iteration 5230/10000, loss = 2.7770349979400635, 2.7692291736602783\n",
      "iteration 5240/10000, loss = 2.774994134902954, 2.7664198875427246\n",
      "iteration 5250/10000, loss = 2.7703475952148438, 2.765444755554199\n",
      "iteration 5260/10000, loss = 2.9064791202545166, 2.7972986698150635\n",
      "iteration 5270/10000, loss = 2.8006913661956787, 2.792346477508545\n",
      "iteration 5280/10000, loss = 2.7810261249542236, 2.775534152984619\n",
      "iteration 5290/10000, loss = 2.776059150695801, 2.7717254161834717\n",
      "iteration 5300/10000, loss = 2.770684003829956, 2.7644503116607666\n",
      "iteration 5310/10000, loss = 2.7892580032348633, 2.785073757171631\n",
      "iteration 5320/10000, loss = 2.776273012161255, 2.773411512374878\n",
      "iteration 5330/10000, loss = 2.7690494060516357, 2.7667272090911865\n",
      "iteration 5340/10000, loss = 2.768440008163452, 2.7664074897766113\n",
      "iteration 5350/10000, loss = 2.788508653640747, 2.8341853618621826\n",
      "iteration 5360/10000, loss = 2.7927768230438232, 2.7866735458374023\n",
      "iteration 5370/10000, loss = 2.7717971801757812, 2.772789716720581\n",
      "iteration 5380/10000, loss = 2.7899837493896484, 2.766892194747925\n",
      "iteration 5390/10000, loss = 2.764155387878418, 2.763977527618408\n",
      "iteration 5400/10000, loss = 2.758488893508911, 2.7554073333740234\n",
      "iteration 5410/10000, loss = 2.8112707138061523, 2.7936198711395264\n",
      "iteration 5420/10000, loss = 2.771554470062256, 2.775449752807617\n",
      "iteration 5430/10000, loss = 2.7599050998687744, 2.758436918258667\n",
      "iteration 5440/10000, loss = 2.772148370742798, 2.7796716690063477\n",
      "iteration 5450/10000, loss = 2.761765956878662, 2.7632968425750732\n",
      "iteration 5460/10000, loss = 2.793372869491577, 2.7778306007385254\n",
      "iteration 5470/10000, loss = 2.7701780796051025, 2.764726400375366\n",
      "iteration 5480/10000, loss = 2.7624964714050293, 2.76387095451355\n",
      "iteration 5490/10000, loss = 2.7658870220184326, 2.777240753173828\n",
      "iteration 5500/10000, loss = 2.7603230476379395, 2.7629921436309814\n",
      "iteration 5510/10000, loss = 2.7625412940979004, 2.761767625808716\n",
      "iteration 5520/10000, loss = 2.7986834049224854, 2.785642147064209\n",
      "iteration 5530/10000, loss = 2.7718405723571777, 2.763390064239502\n",
      "iteration 5540/10000, loss = 2.7577006816864014, 2.747922897338867\n",
      "iteration 5550/10000, loss = 2.7714202404022217, 2.7580413818359375\n",
      "iteration 5560/10000, loss = 2.7627387046813965, 2.7543442249298096\n",
      "iteration 5570/10000, loss = 2.7496626377105713, 2.746051073074341\n",
      "iteration 5580/10000, loss = 2.79070782661438, 2.8006391525268555\n",
      "iteration 5590/10000, loss = 2.774463176727295, 2.7709343433380127\n",
      "iteration 5600/10000, loss = 2.749734878540039, 2.7455060482025146\n",
      "iteration 5610/10000, loss = 2.7476189136505127, 2.7420589923858643\n",
      "iteration 5620/10000, loss = 2.780144214630127, 2.747865915298462\n",
      "iteration 5630/10000, loss = 2.7560248374938965, 2.7432498931884766\n",
      "iteration 5640/10000, loss = 2.754136085510254, 2.758007287979126\n",
      "iteration 5650/10000, loss = 2.7689905166625977, 2.765789031982422\n",
      "iteration 5660/10000, loss = 2.7540078163146973, 2.740791082382202\n",
      "iteration 5670/10000, loss = 2.748534917831421, 2.7488350868225098\n",
      "iteration 5680/10000, loss = 2.756364583969116, 2.772547721862793\n",
      "iteration 5690/10000, loss = 2.753293752670288, 2.73954439163208\n",
      "iteration 5700/10000, loss = 2.8125128746032715, 2.847081184387207\n",
      "iteration 5710/10000, loss = 2.753667116165161, 2.7469818592071533\n",
      "iteration 5720/10000, loss = 2.7542214393615723, 2.7378275394439697\n",
      "iteration 5730/10000, loss = 2.7810893058776855, 2.7861199378967285\n",
      "iteration 5740/10000, loss = 2.756350040435791, 2.756824016571045\n",
      "iteration 5750/10000, loss = 2.744152545928955, 2.7409768104553223\n",
      "iteration 5760/10000, loss = 2.745497226715088, 2.742924928665161\n",
      "iteration 5770/10000, loss = 2.7794275283813477, 2.7641966342926025\n",
      "iteration 5780/10000, loss = 2.745283365249634, 2.751493453979492\n",
      "iteration 5790/10000, loss = 2.755300283432007, 2.7532870769500732\n",
      "iteration 5800/10000, loss = 2.744399070739746, 2.748169183731079\n",
      "iteration 5810/10000, loss = 2.7456483840942383, 2.7396466732025146\n",
      "iteration 5820/10000, loss = 2.739401340484619, 2.738866090774536\n",
      "iteration 5830/10000, loss = 2.7500438690185547, 2.785773992538452\n",
      "iteration 5840/10000, loss = 2.7357406616210938, 2.744297981262207\n",
      "iteration 5850/10000, loss = 2.742605209350586, 2.738354444503784\n",
      "iteration 5860/10000, loss = 2.752586841583252, 2.7366716861724854\n",
      "iteration 5870/10000, loss = 2.738961696624756, 2.7350881099700928\n",
      "iteration 5880/10000, loss = 2.735689163208008, 2.733506202697754\n",
      "iteration 5890/10000, loss = 2.7599050998687744, 2.7683963775634766\n",
      "iteration 5900/10000, loss = 2.7428221702575684, 2.734705924987793\n",
      "iteration 5910/10000, loss = 2.7396764755249023, 2.732696294784546\n",
      "iteration 5920/10000, loss = 2.7669787406921387, 2.782604932785034\n",
      "iteration 5930/10000, loss = 2.7362265586853027, 2.7305331230163574\n",
      "iteration 5940/10000, loss = 2.730311393737793, 2.7239863872528076\n",
      "iteration 5950/10000, loss = 2.7266550064086914, 2.7189323902130127\n",
      "iteration 5960/10000, loss = 2.7646753787994385, 2.7832555770874023\n",
      "iteration 5970/10000, loss = 2.728557586669922, 2.7274487018585205\n",
      "iteration 5980/10000, loss = 2.7243587970733643, 2.7196004390716553\n",
      "iteration 5990/10000, loss = 2.7552037239074707, 2.7685253620147705\n",
      "iteration 6000/10000, loss = 2.7338104248046875, 2.7440035343170166\n",
      "iteration 6010/10000, loss = 2.7362167835235596, 2.7224316596984863\n",
      "iteration 6020/10000, loss = 2.7607617378234863, 2.781780958175659\n",
      "iteration 6030/10000, loss = 2.7306888103485107, 2.7310800552368164\n",
      "iteration 6040/10000, loss = 2.729645252227783, 2.72969651222229\n",
      "iteration 6050/10000, loss = 2.7281408309936523, 2.7204084396362305\n",
      "iteration 6060/10000, loss = 2.75246000289917, 2.774714708328247\n",
      "iteration 6070/10000, loss = 2.765181303024292, 2.7381272315979004\n",
      "iteration 6080/10000, loss = 2.723477840423584, 2.7307240962982178\n",
      "iteration 6090/10000, loss = 2.719114303588867, 2.719316005706787\n",
      "iteration 6100/10000, loss = 2.721700668334961, 2.7138164043426514\n",
      "iteration 6110/10000, loss = 2.7298073768615723, 2.7350881099700928\n",
      "iteration 6120/10000, loss = 2.772088050842285, 2.729524612426758\n",
      "iteration 6130/10000, loss = 2.7290892601013184, 2.7306110858917236\n",
      "iteration 6140/10000, loss = 2.7275784015655518, 2.7142486572265625\n",
      "iteration 6150/10000, loss = 2.716952323913574, 2.7091996669769287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6160/10000, loss = 2.7130556106567383, 2.7102890014648438\n",
      "iteration 6170/10000, loss = 2.859894037246704, 2.746770143508911\n",
      "iteration 6180/10000, loss = 2.7506139278411865, 2.7360029220581055\n",
      "iteration 6190/10000, loss = 2.726123571395874, 2.718583822250366\n",
      "iteration 6200/10000, loss = 2.731329917907715, 2.729346990585327\n",
      "iteration 6210/10000, loss = 2.7193105220794678, 2.712287187576294\n",
      "iteration 6220/10000, loss = 2.713134765625, 2.7096431255340576\n",
      "iteration 6230/10000, loss = 2.7320141792297363, 2.7182681560516357\n",
      "iteration 6240/10000, loss = 2.707990884780884, 2.701953411102295\n",
      "iteration 6250/10000, loss = 2.7214181423187256, 2.7183637619018555\n",
      "iteration 6260/10000, loss = 2.7716898918151855, 2.7521705627441406\n",
      "iteration 6270/10000, loss = 2.742169141769409, 2.723719358444214\n",
      "iteration 6280/10000, loss = 2.717039108276367, 2.718897819519043\n",
      "iteration 6290/10000, loss = 2.7141644954681396, 2.7196574211120605\n",
      "iteration 6300/10000, loss = 2.7151076793670654, 2.7040560245513916\n",
      "iteration 6310/10000, loss = 2.7199041843414307, 2.7138984203338623\n",
      "iteration 6320/10000, loss = 2.7043471336364746, 2.7011539936065674\n",
      "iteration 6330/10000, loss = 2.782567024230957, 2.7511303424835205\n",
      "iteration 6340/10000, loss = 2.7347826957702637, 2.7182092666625977\n",
      "iteration 6350/10000, loss = 2.7071115970611572, 2.70491361618042\n",
      "iteration 6360/10000, loss = 2.7324414253234863, 2.7384135723114014\n",
      "iteration 6370/10000, loss = 2.7107295989990234, 2.698854446411133\n",
      "iteration 6380/10000, loss = 2.736002206802368, 2.727139949798584\n",
      "iteration 6390/10000, loss = 2.7109527587890625, 2.7004594802856445\n",
      "iteration 6400/10000, loss = 2.7388439178466797, 2.7409892082214355\n",
      "iteration 6410/10000, loss = 2.7109482288360596, 2.697634696960449\n",
      "iteration 6420/10000, loss = 2.703463554382324, 2.700812578201294\n",
      "iteration 6430/10000, loss = 2.8464605808258057, 2.8635008335113525\n",
      "iteration 6440/10000, loss = 2.770084857940674, 2.7320635318756104\n",
      "iteration 6450/10000, loss = 2.7192270755767822, 2.7015132904052734\n",
      "iteration 6460/10000, loss = 2.746147394180298, 2.751187801361084\n",
      "iteration 6470/10000, loss = 2.7031006813049316, 2.70670485496521\n",
      "iteration 6480/10000, loss = 2.699904680252075, 2.6909379959106445\n",
      "iteration 6490/10000, loss = 2.696826696395874, 2.6906208992004395\n",
      "iteration 6500/10000, loss = 2.7021374702453613, 2.7023603916168213\n",
      "iteration 6510/10000, loss = 2.74651837348938, 2.7052536010742188\n",
      "iteration 6520/10000, loss = 2.717684745788574, 2.7008066177368164\n",
      "iteration 6530/10000, loss = 2.7074108123779297, 2.7041268348693848\n",
      "iteration 6540/10000, loss = 2.7422704696655273, 2.694622278213501\n",
      "iteration 6550/10000, loss = 2.7144153118133545, 2.6943044662475586\n",
      "iteration 6560/10000, loss = 2.69388747215271, 2.6891274452209473\n",
      "iteration 6570/10000, loss = 2.7382023334503174, 2.7424733638763428\n",
      "iteration 6580/10000, loss = 2.698822498321533, 2.704082489013672\n",
      "iteration 6590/10000, loss = 2.6926043033599854, 2.6913270950317383\n",
      "iteration 6600/10000, loss = 2.694890260696411, 2.6899778842926025\n",
      "iteration 6610/10000, loss = 2.823162794113159, 2.885826826095581\n",
      "iteration 6620/10000, loss = 2.729740619659424, 2.715773344039917\n",
      "iteration 6630/10000, loss = 2.705007791519165, 2.6977553367614746\n",
      "iteration 6640/10000, loss = 2.7684874534606934, 2.752760648727417\n",
      "iteration 6650/10000, loss = 2.712388277053833, 2.6903507709503174\n",
      "iteration 6660/10000, loss = 2.69291353225708, 2.6820919513702393\n",
      "iteration 6670/10000, loss = 2.68892502784729, 2.685487985610962\n",
      "iteration 6680/10000, loss = 2.729311466217041, 2.722137928009033\n",
      "iteration 6690/10000, loss = 2.691561460494995, 2.690267324447632\n",
      "iteration 6700/10000, loss = 2.695559501647949, 2.690734386444092\n",
      "iteration 6710/10000, loss = 2.6920671463012695, 2.694307327270508\n",
      "iteration 6720/10000, loss = 2.7487194538116455, 2.6930644512176514\n",
      "iteration 6730/10000, loss = 2.6942009925842285, 2.6946451663970947\n",
      "iteration 6740/10000, loss = 2.710310697555542, 2.7093443870544434\n",
      "iteration 6750/10000, loss = 2.698450803756714, 2.6888723373413086\n",
      "iteration 6760/10000, loss = 2.6958744525909424, 2.694222927093506\n",
      "iteration 6770/10000, loss = 2.685991048812866, 2.6831560134887695\n",
      "iteration 6780/10000, loss = 2.685614585876465, 2.6788320541381836\n",
      "iteration 6790/10000, loss = 2.729700803756714, 2.752912759780884\n",
      "iteration 6800/10000, loss = 2.6859986782073975, 2.7052371501922607\n",
      "iteration 6810/10000, loss = 2.685638666152954, 2.6857638359069824\n",
      "iteration 6820/10000, loss = 2.6877968311309814, 2.6823372840881348\n",
      "iteration 6830/10000, loss = 2.6840436458587646, 2.6815006732940674\n",
      "iteration 6840/10000, loss = 2.6910269260406494, 2.6801443099975586\n",
      "iteration 6850/10000, loss = 2.6831133365631104, 2.677915334701538\n",
      "iteration 6860/10000, loss = 2.6931843757629395, 2.687098979949951\n",
      "iteration 6870/10000, loss = 2.688777208328247, 2.6944594383239746\n",
      "iteration 6880/10000, loss = 2.7117714881896973, 2.698647975921631\n",
      "iteration 6890/10000, loss = 2.684048652648926, 2.6890642642974854\n",
      "iteration 6900/10000, loss = 2.6972124576568604, 2.6929330825805664\n",
      "iteration 6910/10000, loss = 2.706815242767334, 2.6760377883911133\n",
      "iteration 6920/10000, loss = 2.6773970127105713, 2.678910493850708\n",
      "iteration 6930/10000, loss = 2.6754393577575684, 2.6682534217834473\n",
      "iteration 6940/10000, loss = 2.6727325916290283, 2.667365550994873\n",
      "iteration 6950/10000, loss = 2.7985098361968994, 2.7711565494537354\n",
      "iteration 6960/10000, loss = 2.6757771968841553, 2.682474374771118\n",
      "iteration 6970/10000, loss = 2.6843466758728027, 2.678649425506592\n",
      "iteration 6980/10000, loss = 2.7214760780334473, 2.7457308769226074\n",
      "iteration 6990/10000, loss = 2.713937282562256, 2.6852903366088867\n",
      "iteration 7000/10000, loss = 2.6803462505340576, 2.6819770336151123\n",
      "iteration 7010/10000, loss = 2.6733524799346924, 2.670748472213745\n",
      "iteration 7020/10000, loss = 2.747981309890747, 2.772857904434204\n",
      "iteration 7030/10000, loss = 2.7028794288635254, 2.669168472290039\n",
      "iteration 7040/10000, loss = 2.6849868297576904, 2.678548574447632\n",
      "iteration 7050/10000, loss = 2.6798059940338135, 2.674513816833496\n",
      "iteration 7060/10000, loss = 2.729949712753296, 2.7243335247039795\n",
      "iteration 7070/10000, loss = 2.70320463180542, 2.6744823455810547\n",
      "iteration 7080/10000, loss = 2.6795122623443604, 2.675888776779175\n",
      "iteration 7090/10000, loss = 2.6855742931365967, 2.686394453048706\n",
      "iteration 7100/10000, loss = 2.6852643489837646, 2.6786866188049316\n",
      "iteration 7110/10000, loss = 2.6655590534210205, 2.662792205810547\n",
      "iteration 7120/10000, loss = 2.6654717922210693, 2.6616551876068115\n",
      "iteration 7130/10000, loss = 2.7153775691986084, 2.720074415206909\n",
      "iteration 7140/10000, loss = 2.6802561283111572, 2.680426836013794\n",
      "iteration 7150/10000, loss = 2.676917552947998, 2.675478219985962\n",
      "iteration 7160/10000, loss = 2.672665596008301, 2.6659083366394043\n",
      "iteration 7170/10000, loss = 2.744227886199951, 2.703685760498047\n",
      "iteration 7180/10000, loss = 2.670319080352783, 2.663421630859375\n",
      "iteration 7190/10000, loss = 2.663512945175171, 2.656282663345337\n",
      "iteration 7200/10000, loss = 2.6632301807403564, 2.659472703933716\n",
      "iteration 7210/10000, loss = 2.8129818439483643, 2.952000379562378\n",
      "iteration 7220/10000, loss = 2.7157764434814453, 2.7010011672973633\n",
      "iteration 7230/10000, loss = 2.6854026317596436, 2.672814130783081\n",
      "iteration 7240/10000, loss = 2.6706886291503906, 2.658785343170166\n",
      "iteration 7250/10000, loss = 2.687683582305908, 2.6964809894561768\n",
      "iteration 7260/10000, loss = 2.6682941913604736, 2.6568684577941895\n",
      "iteration 7270/10000, loss = 2.6650404930114746, 2.6584463119506836\n",
      "iteration 7280/10000, loss = 2.6642396450042725, 2.659383773803711\n",
      "iteration 7290/10000, loss = 2.6701393127441406, 2.6731722354888916\n",
      "iteration 7300/10000, loss = 2.69046950340271, 2.66748309135437\n",
      "iteration 7310/10000, loss = 2.6700632572174072, 2.6613783836364746\n",
      "iteration 7320/10000, loss = 2.7243504524230957, 2.7108230590820312\n",
      "iteration 7330/10000, loss = 2.6767964363098145, 2.660874128341675\n",
      "iteration 7340/10000, loss = 2.6666128635406494, 2.6586594581604004\n",
      "iteration 7350/10000, loss = 2.680866241455078, 2.6655588150024414\n",
      "iteration 7360/10000, loss = 2.6625826358795166, 2.6607236862182617\n",
      "iteration 7370/10000, loss = 2.664325714111328, 2.6630606651306152\n",
      "iteration 7380/10000, loss = 2.7504193782806396, 2.743987798690796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7390/10000, loss = 2.6629254817962646, 2.6527881622314453\n",
      "iteration 7400/10000, loss = 2.6567647457122803, 2.6582369804382324\n",
      "iteration 7410/10000, loss = 2.7766804695129395, 2.8295135498046875\n",
      "iteration 7420/10000, loss = 2.700693130493164, 2.705155611038208\n",
      "iteration 7430/10000, loss = 2.6806247234344482, 2.6516330242156982\n",
      "iteration 7440/10000, loss = 2.666224241256714, 2.650665760040283\n",
      "iteration 7450/10000, loss = 2.6539294719696045, 2.651287078857422\n",
      "iteration 7460/10000, loss = 2.666370153427124, 2.663177013397217\n",
      "iteration 7470/10000, loss = 2.654097318649292, 2.6487765312194824\n",
      "iteration 7480/10000, loss = 2.667132616043091, 2.6637001037597656\n",
      "iteration 7490/10000, loss = 2.653175115585327, 2.651279926300049\n",
      "iteration 7500/10000, loss = 2.6579556465148926, 2.6582047939300537\n",
      "iteration 7510/10000, loss = 2.7300851345062256, 2.7266273498535156\n",
      "iteration 7520/10000, loss = 2.6777002811431885, 2.6543235778808594\n",
      "iteration 7530/10000, loss = 2.661255121231079, 2.652024030685425\n",
      "iteration 7540/10000, loss = 2.6502695083618164, 2.64227557182312\n",
      "iteration 7550/10000, loss = 2.7945468425750732, 2.8331809043884277\n",
      "iteration 7560/10000, loss = 2.6998486518859863, 2.6780688762664795\n",
      "iteration 7570/10000, loss = 2.669182538986206, 2.6633100509643555\n",
      "iteration 7580/10000, loss = 2.6546413898468018, 2.6417176723480225\n",
      "iteration 7590/10000, loss = 2.6500179767608643, 2.643742561340332\n",
      "iteration 7600/10000, loss = 2.648038625717163, 2.6435506343841553\n",
      "iteration 7610/10000, loss = 2.668003559112549, 2.6703617572784424\n",
      "iteration 7620/10000, loss = 2.661928415298462, 2.651071786880493\n",
      "iteration 7630/10000, loss = 2.649387836456299, 2.6449875831604004\n",
      "iteration 7640/10000, loss = 2.669623613357544, 2.6784722805023193\n",
      "iteration 7650/10000, loss = 2.6571125984191895, 2.67334246635437\n",
      "iteration 7660/10000, loss = 2.6626670360565186, 2.653055191040039\n",
      "iteration 7670/10000, loss = 2.647932529449463, 2.645151138305664\n",
      "iteration 7680/10000, loss = 2.646650791168213, 2.640350341796875\n",
      "iteration 7690/10000, loss = 2.696333646774292, 2.7435507774353027\n",
      "iteration 7700/10000, loss = 2.69814133644104, 2.6856679916381836\n",
      "iteration 7710/10000, loss = 2.661370038986206, 2.646303176879883\n",
      "iteration 7720/10000, loss = 2.643079996109009, 2.6368236541748047\n",
      "iteration 7730/10000, loss = 2.6472461223602295, 2.6401355266571045\n",
      "iteration 7740/10000, loss = 2.640580654144287, 2.634169340133667\n",
      "iteration 7750/10000, loss = 2.6933460235595703, 2.703314781188965\n",
      "iteration 7760/10000, loss = 2.6465940475463867, 2.639967441558838\n",
      "iteration 7770/10000, loss = 2.642873525619507, 2.6348679065704346\n",
      "iteration 7780/10000, loss = 2.652941942214966, 2.645998239517212\n",
      "iteration 7790/10000, loss = 2.7074224948883057, 2.730098009109497\n",
      "iteration 7800/10000, loss = 2.667879819869995, 2.651454210281372\n",
      "iteration 7810/10000, loss = 2.650575637817383, 2.6398141384124756\n",
      "iteration 7820/10000, loss = 2.6330924034118652, 2.6276931762695312\n",
      "iteration 7830/10000, loss = 2.7025187015533447, 2.7551887035369873\n",
      "iteration 7840/10000, loss = 2.661060094833374, 2.65390944480896\n",
      "iteration 7850/10000, loss = 2.6418869495391846, 2.644308090209961\n",
      "iteration 7860/10000, loss = 2.642699718475342, 2.635913372039795\n",
      "iteration 7870/10000, loss = 2.655222177505493, 2.6774537563323975\n",
      "iteration 7880/10000, loss = 2.6452770233154297, 2.650275707244873\n",
      "iteration 7890/10000, loss = 2.6452219486236572, 2.6338260173797607\n",
      "iteration 7900/10000, loss = 2.633239507675171, 2.6300694942474365\n",
      "iteration 7910/10000, loss = 2.6520750522613525, 2.6578118801116943\n",
      "iteration 7920/10000, loss = 2.656852960586548, 2.653827428817749\n",
      "iteration 7930/10000, loss = 2.6402642726898193, 2.6280553340911865\n",
      "iteration 7940/10000, loss = 2.6527364253997803, 2.648371458053589\n",
      "iteration 7950/10000, loss = 2.631413698196411, 2.6301355361938477\n",
      "iteration 7960/10000, loss = 2.644916534423828, 2.6244771480560303\n",
      "iteration 7970/10000, loss = 2.639063596725464, 2.640632152557373\n",
      "iteration 7980/10000, loss = 2.631373643875122, 2.6293983459472656\n",
      "iteration 7990/10000, loss = 2.6492536067962646, 2.6313557624816895\n",
      "iteration 8000/10000, loss = 2.648236036300659, 2.645320415496826\n",
      "iteration 8010/10000, loss = 2.6355745792388916, 2.62737774848938\n",
      "iteration 8020/10000, loss = 2.651766061782837, 2.6680688858032227\n",
      "iteration 8030/10000, loss = 2.640493392944336, 2.650728940963745\n",
      "iteration 8040/10000, loss = 2.646477222442627, 2.631873607635498\n",
      "iteration 8050/10000, loss = 2.636977434158325, 2.6314022541046143\n",
      "iteration 8060/10000, loss = 2.630223035812378, 2.6356775760650635\n",
      "iteration 8070/10000, loss = 2.626960515975952, 2.6188037395477295\n",
      "iteration 8080/10000, loss = 2.6226704120635986, 2.6163971424102783\n",
      "iteration 8090/10000, loss = 2.6671142578125, 2.677762508392334\n",
      "iteration 8100/10000, loss = 2.66888165473938, 2.7111315727233887\n",
      "iteration 8110/10000, loss = 2.6735570430755615, 2.6682381629943848\n",
      "iteration 8120/10000, loss = 2.6296515464782715, 2.638944625854492\n",
      "iteration 8130/10000, loss = 2.633021831512451, 2.629392623901367\n",
      "iteration 8140/10000, loss = 2.653122901916504, 2.638625383377075\n",
      "iteration 8150/10000, loss = 2.631826639175415, 2.627096652984619\n",
      "iteration 8160/10000, loss = 2.6289710998535156, 2.623356342315674\n",
      "iteration 8170/10000, loss = 2.658379554748535, 2.6811141967773438\n",
      "iteration 8180/10000, loss = 2.672264575958252, 2.6599934101104736\n",
      "iteration 8190/10000, loss = 2.6215293407440186, 2.623720169067383\n",
      "iteration 8200/10000, loss = 2.618359088897705, 2.614835739135742\n",
      "iteration 8210/10000, loss = 2.6689693927764893, 2.693269729614258\n",
      "iteration 8220/10000, loss = 2.6402485370635986, 2.6466362476348877\n",
      "iteration 8230/10000, loss = 2.6340792179107666, 2.6229138374328613\n",
      "iteration 8240/10000, loss = 2.620065927505493, 2.618605136871338\n",
      "iteration 8250/10000, loss = 2.625011444091797, 2.6651957035064697\n",
      "iteration 8260/10000, loss = 2.6251754760742188, 2.633620262145996\n",
      "iteration 8270/10000, loss = 2.626946449279785, 2.6158535480499268\n",
      "iteration 8280/10000, loss = 2.6175594329833984, 2.6135809421539307\n",
      "iteration 8290/10000, loss = 2.788494110107422, 2.6351399421691895\n",
      "iteration 8300/10000, loss = 2.670440196990967, 2.6414225101470947\n",
      "iteration 8310/10000, loss = 2.6325714588165283, 2.6155848503112793\n",
      "iteration 8320/10000, loss = 2.6231184005737305, 2.62113881111145\n",
      "iteration 8330/10000, loss = 2.6277263164520264, 2.6430723667144775\n",
      "iteration 8340/10000, loss = 2.6235923767089844, 2.612001657485962\n",
      "iteration 8350/10000, loss = 2.6158971786499023, 2.6077563762664795\n",
      "iteration 8360/10000, loss = 2.6158292293548584, 2.6124205589294434\n",
      "iteration 8370/10000, loss = 2.6246497631073, 2.6263763904571533\n",
      "iteration 8380/10000, loss = 2.6454896926879883, 2.63448429107666\n",
      "iteration 8390/10000, loss = 2.6171836853027344, 2.6155765056610107\n",
      "iteration 8400/10000, loss = 2.616894245147705, 2.611462116241455\n",
      "iteration 8410/10000, loss = 2.7653815746307373, 2.8448588848114014\n",
      "iteration 8420/10000, loss = 2.6822757720947266, 2.63883900642395\n",
      "iteration 8430/10000, loss = 2.6331896781921387, 2.6344709396362305\n",
      "iteration 8440/10000, loss = 2.6195130348205566, 2.6179416179656982\n",
      "iteration 8450/10000, loss = 2.6164674758911133, 2.6097774505615234\n",
      "iteration 8460/10000, loss = 2.6365244388580322, 2.616969585418701\n",
      "iteration 8470/10000, loss = 2.6151697635650635, 2.610166072845459\n",
      "iteration 8480/10000, loss = 2.613255262374878, 2.606182813644409\n",
      "iteration 8490/10000, loss = 2.6176364421844482, 2.6227529048919678\n",
      "iteration 8500/10000, loss = 2.7021186351776123, 2.6273982524871826\n",
      "iteration 8510/10000, loss = 2.6352524757385254, 2.609642744064331\n",
      "iteration 8520/10000, loss = 2.6151435375213623, 2.612335205078125\n",
      "iteration 8530/10000, loss = 2.6091837882995605, 2.6051502227783203\n",
      "iteration 8540/10000, loss = 2.6071417331695557, 2.601482629776001\n",
      "iteration 8550/10000, loss = 2.6064343452453613, 2.6036970615386963\n",
      "iteration 8560/10000, loss = 2.7006077766418457, 2.717311382293701\n",
      "iteration 8570/10000, loss = 2.6449270248413086, 2.6392626762390137\n",
      "iteration 8580/10000, loss = 2.62595272064209, 2.6043968200683594\n",
      "iteration 8590/10000, loss = 2.6112635135650635, 2.608987808227539\n",
      "iteration 8600/10000, loss = 2.627424955368042, 2.679723024368286\n",
      "iteration 8610/10000, loss = 2.6335208415985107, 2.6374049186706543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8620/10000, loss = 2.6111104488372803, 2.615143060684204\n",
      "iteration 8630/10000, loss = 2.610278844833374, 2.6001882553100586\n",
      "iteration 8640/10000, loss = 2.6120357513427734, 2.6102631092071533\n",
      "iteration 8650/10000, loss = 2.6020348072052, 2.5985934734344482\n",
      "iteration 8660/10000, loss = 2.604703903198242, 2.6006717681884766\n",
      "iteration 8670/10000, loss = 2.615433931350708, 2.6093599796295166\n",
      "iteration 8680/10000, loss = 2.624204158782959, 2.6415576934814453\n",
      "iteration 8690/10000, loss = 2.609431028366089, 2.639204502105713\n",
      "iteration 8700/10000, loss = 2.6021931171417236, 2.602445602416992\n",
      "iteration 8710/10000, loss = 2.612290859222412, 2.62268328666687\n",
      "iteration 8720/10000, loss = 2.6293842792510986, 2.6763267517089844\n",
      "iteration 8730/10000, loss = 2.6197664737701416, 2.6119701862335205\n",
      "iteration 8740/10000, loss = 2.6062097549438477, 2.597557783126831\n",
      "iteration 8750/10000, loss = 2.5995965003967285, 2.5960075855255127\n",
      "iteration 8760/10000, loss = 2.621101140975952, 2.6281466484069824\n",
      "iteration 8770/10000, loss = 2.6200976371765137, 2.5975069999694824\n",
      "iteration 8780/10000, loss = 2.605684995651245, 2.5941121578216553\n",
      "iteration 8790/10000, loss = 2.601471185684204, 2.601857900619507\n",
      "iteration 8800/10000, loss = 2.6248090267181396, 2.635702133178711\n",
      "iteration 8810/10000, loss = 2.6195247173309326, 2.605123281478882\n",
      "iteration 8820/10000, loss = 2.6021084785461426, 2.6003360748291016\n",
      "iteration 8830/10000, loss = 2.665027379989624, 2.6279118061065674\n",
      "iteration 8840/10000, loss = 2.6016974449157715, 2.5940468311309814\n",
      "iteration 8850/10000, loss = 2.6054670810699463, 2.59912109375\n",
      "iteration 8860/10000, loss = 2.603410005569458, 2.6004209518432617\n",
      "iteration 8870/10000, loss = 2.595557928085327, 2.588301420211792\n",
      "iteration 8880/10000, loss = 2.6812949180603027, 2.6269612312316895\n",
      "iteration 8890/10000, loss = 2.6310980319976807, 2.59919810295105\n",
      "iteration 8900/10000, loss = 2.592639684677124, 2.594167470932007\n",
      "iteration 8910/10000, loss = 2.614254951477051, 2.607846260070801\n",
      "iteration 8920/10000, loss = 2.5961902141571045, 2.5937230587005615\n",
      "iteration 8930/10000, loss = 2.6341817378997803, 2.6419413089752197\n",
      "iteration 8940/10000, loss = 2.5924072265625, 2.5994913578033447\n",
      "iteration 8950/10000, loss = 2.5945446491241455, 2.5847628116607666\n",
      "iteration 8960/10000, loss = 2.6083498001098633, 2.623704671859741\n",
      "iteration 8970/10000, loss = 2.5996291637420654, 2.633889675140381\n",
      "iteration 8980/10000, loss = 2.594179391860962, 2.601938486099243\n",
      "iteration 8990/10000, loss = 2.6009812355041504, 2.5934653282165527\n",
      "iteration 9000/10000, loss = 2.6792428493499756, 2.6029908657073975\n",
      "iteration 9010/10000, loss = 2.615992784500122, 2.5981485843658447\n",
      "iteration 9020/10000, loss = 2.5917415618896484, 2.5835952758789062\n",
      "iteration 9030/10000, loss = 2.595773220062256, 2.593341112136841\n",
      "iteration 9040/10000, loss = 2.6029052734375, 2.64591646194458\n",
      "iteration 9050/10000, loss = 2.590306282043457, 2.597970962524414\n",
      "iteration 9060/10000, loss = 2.59871768951416, 2.5882129669189453\n",
      "iteration 9070/10000, loss = 2.6190900802612305, 2.6461360454559326\n",
      "iteration 9080/10000, loss = 2.667616605758667, 2.620518684387207\n",
      "iteration 9090/10000, loss = 2.597599744796753, 2.592827796936035\n",
      "iteration 9100/10000, loss = 2.596296787261963, 2.583237409591675\n",
      "iteration 9110/10000, loss = 2.58868408203125, 2.58231782913208\n",
      "iteration 9120/10000, loss = 2.6607437133789062, 2.60530948638916\n",
      "iteration 9130/10000, loss = 2.5864386558532715, 2.5941274166107178\n",
      "iteration 9140/10000, loss = 2.58774733543396, 2.588200092315674\n",
      "iteration 9150/10000, loss = 2.59840726852417, 2.600977897644043\n",
      "iteration 9160/10000, loss = 2.605639934539795, 2.5971577167510986\n",
      "iteration 9170/10000, loss = 2.5972554683685303, 2.5842719078063965\n",
      "iteration 9180/10000, loss = 2.5809216499328613, 2.5753400325775146\n",
      "iteration 9190/10000, loss = 2.6008083820343018, 2.5988330841064453\n",
      "iteration 9200/10000, loss = 2.602102041244507, 2.610668897628784\n",
      "iteration 9210/10000, loss = 2.6051383018493652, 2.5858893394470215\n",
      "iteration 9220/10000, loss = 2.5855014324188232, 2.584369659423828\n",
      "iteration 9230/10000, loss = 2.760040760040283, 2.820956230163574\n",
      "iteration 9240/10000, loss = 2.6372766494750977, 2.594136953353882\n",
      "iteration 9250/10000, loss = 2.590921640396118, 2.5875589847564697\n",
      "iteration 9260/10000, loss = 2.5917787551879883, 2.5899264812469482\n",
      "iteration 9270/10000, loss = 2.6040103435516357, 2.578434944152832\n",
      "iteration 9280/10000, loss = 2.5912015438079834, 2.580815076828003\n",
      "iteration 9290/10000, loss = 2.5833539962768555, 2.580768346786499\n",
      "iteration 9300/10000, loss = 2.591073751449585, 2.5965442657470703\n",
      "iteration 9310/10000, loss = 2.609973430633545, 2.590515375137329\n",
      "iteration 9320/10000, loss = 2.5780279636383057, 2.576868772506714\n",
      "iteration 9330/10000, loss = 2.58355450630188, 2.5743038654327393\n",
      "iteration 9340/10000, loss = 2.5821709632873535, 2.5768723487854004\n",
      "iteration 9350/10000, loss = 2.626666784286499, 2.5971457958221436\n",
      "iteration 9360/10000, loss = 2.5999643802642822, 2.578232526779175\n",
      "iteration 9370/10000, loss = 2.5778048038482666, 2.5764503479003906\n",
      "iteration 9380/10000, loss = 2.6157896518707275, 2.6096127033233643\n",
      "iteration 9390/10000, loss = 2.5912704467773438, 2.5806751251220703\n",
      "iteration 9400/10000, loss = 2.5796279907226562, 2.5771424770355225\n",
      "iteration 9410/10000, loss = 2.6064035892486572, 2.595400810241699\n",
      "iteration 9420/10000, loss = 2.5803890228271484, 2.5793232917785645\n",
      "iteration 9430/10000, loss = 2.585052013397217, 2.577993154525757\n",
      "iteration 9440/10000, loss = 2.6061267852783203, 2.5814707279205322\n",
      "iteration 9450/10000, loss = 2.588381767272949, 2.5784523487091064\n",
      "iteration 9460/10000, loss = 2.5800094604492188, 2.5798556804656982\n",
      "iteration 9470/10000, loss = 2.6362216472625732, 2.5997633934020996\n",
      "iteration 9480/10000, loss = 2.600177526473999, 2.593932628631592\n",
      "iteration 9490/10000, loss = 2.5864524841308594, 2.577975273132324\n",
      "iteration 9500/10000, loss = 2.5861644744873047, 2.5949628353118896\n",
      "iteration 9510/10000, loss = 2.5881080627441406, 2.5836870670318604\n",
      "iteration 9520/10000, loss = 2.5739905834198, 2.5737972259521484\n",
      "iteration 9530/10000, loss = 2.617636203765869, 2.6351709365844727\n",
      "iteration 9540/10000, loss = 2.588838815689087, 2.597670793533325\n",
      "iteration 9550/10000, loss = 2.5840530395507812, 2.580510377883911\n",
      "iteration 9560/10000, loss = 2.5676565170288086, 2.563723564147949\n",
      "iteration 9570/10000, loss = 2.694460868835449, 2.6582086086273193\n",
      "iteration 9580/10000, loss = 2.6061856746673584, 2.6025006771087646\n",
      "iteration 9590/10000, loss = 2.576369524002075, 2.5638434886932373\n",
      "iteration 9600/10000, loss = 2.575371503829956, 2.566558361053467\n",
      "iteration 9610/10000, loss = 2.568023681640625, 2.561903953552246\n",
      "iteration 9620/10000, loss = 2.7460012435913086, 2.7584190368652344\n",
      "iteration 9630/10000, loss = 2.6268908977508545, 2.5974185466766357\n",
      "iteration 9640/10000, loss = 2.586017370223999, 2.586135149002075\n",
      "iteration 9650/10000, loss = 2.5767407417297363, 2.564694881439209\n",
      "iteration 9660/10000, loss = 2.568302869796753, 2.5649561882019043\n",
      "iteration 9670/10000, loss = 2.573317289352417, 2.573521614074707\n",
      "iteration 9680/10000, loss = 2.6109485626220703, 2.6107497215270996\n",
      "iteration 9690/10000, loss = 2.581543207168579, 2.584437608718872\n",
      "iteration 9700/10000, loss = 2.577564001083374, 2.567094564437866\n",
      "iteration 9710/10000, loss = 2.571837902069092, 2.570566415786743\n",
      "iteration 9720/10000, loss = 2.5823614597320557, 2.563798189163208\n",
      "iteration 9730/10000, loss = 2.5769855976104736, 2.577082633972168\n",
      "iteration 9740/10000, loss = 2.5819694995880127, 2.578627109527588\n",
      "iteration 9750/10000, loss = 2.61767840385437, 2.6103386878967285\n",
      "iteration 9760/10000, loss = 2.574446201324463, 2.5710926055908203\n",
      "iteration 9770/10000, loss = 2.578354597091675, 2.585209846496582\n",
      "iteration 9780/10000, loss = 2.5972816944122314, 2.5607213973999023\n",
      "iteration 9790/10000, loss = 2.57132887840271, 2.57423734664917\n",
      "iteration 9800/10000, loss = 2.582118511199951, 2.588715076446533\n",
      "iteration 9810/10000, loss = 2.598310708999634, 2.570124626159668\n",
      "iteration 9820/10000, loss = 2.5604617595672607, 2.5607492923736572\n",
      "iteration 9830/10000, loss = 2.5640575885772705, 2.556107759475708\n",
      "iteration 9840/10000, loss = 2.6993014812469482, 2.695110559463501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9850/10000, loss = 2.5675530433654785, 2.5936074256896973\n",
      "iteration 9860/10000, loss = 2.5674784183502197, 2.5545620918273926\n",
      "iteration 9870/10000, loss = 2.5587048530578613, 2.5544183254241943\n",
      "iteration 9880/10000, loss = 2.577502965927124, 2.585887908935547\n",
      "iteration 9890/10000, loss = 2.578519582748413, 2.5778770446777344\n",
      "iteration 9900/10000, loss = 2.557889699935913, 2.5618977546691895\n",
      "iteration 9910/10000, loss = 2.5598435401916504, 2.551727771759033\n",
      "iteration 9920/10000, loss = 2.5581538677215576, 2.553149700164795\n",
      "iteration 9930/10000, loss = 2.646206855773926, 2.631617546081543\n",
      "iteration 9940/10000, loss = 2.6009984016418457, 2.5705387592315674\n",
      "iteration 9950/10000, loss = 2.569537401199341, 2.5612640380859375\n",
      "iteration 9960/10000, loss = 2.562255620956421, 2.5597047805786133\n",
      "iteration 9970/10000, loss = 2.556330680847168, 2.553394079208374\n",
      "iteration 9980/10000, loss = 2.5863230228424072, 2.590932846069336\n",
      "iteration 9990/10000, loss = 2.5736472606658936, 2.5710339546203613\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "x = np.hstack([a, s])                                # -> (N, 23)\n",
    "y = np.hstack([s_new, np.array(r).reshape(-1, 1)])   # -> (N, 18)\n",
    "\n",
    "# converting to tensors\n",
    "x = torch.tensor(x, dtype=torch.float32)   \n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "# info\n",
    "print(\n",
    "    f\"x_train shape = {x_train.shape}\\n\" \\\n",
    "    f\"x_test shape = {x_test.shape}\\n\" \\\n",
    "    f\"y_train shape = {y_train.shape}\\n\" \\\n",
    "    f\"y_test shape = {y_test.shape}\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "x_train = x_train.to(device)\n",
    "x_test = x_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# set_data\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)\n",
    "\n",
    "# set params\n",
    "epochs = 10000\n",
    "\n",
    "# for network_width in width_list:\n",
    "model = Network( h_size=50 )\n",
    "model = model.to(device)\n",
    "\n",
    "# train\n",
    "train_losses, test_losses = model.train(train_data, epochs=epochs, cp=10)\n",
    "# test_results.append(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aeb9e4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot data\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.arange(1000, epochs)\n",
    "ax.plot(x, train_losses[1000:], label='train', color='red')\n",
    "ax.plot(x, test_losses[1000:], label='test', color='green')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892327a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
