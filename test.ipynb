{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9251f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537bb78",
   "metadata": {},
   "source": [
    "# Gather Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9d576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_random_data(env, steps=5000):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "\n",
    "    s, _ = env.reset()\n",
    "    for _ in range(steps):\n",
    "        a = env.action_space.sample()\n",
    "\n",
    "        s_new, r, done, _, _ = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        next_states.append(s_new)\n",
    "\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "        else:\n",
    "            s = s_new\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions).reshape(-1, 1)\n",
    "    inputs = np.concatenate([states, actions], axis=1)\n",
    "    next_states = np.array(next_states)\n",
    "    rewards = np.array(rewards).reshape(-1, 1)\n",
    "    # convert to tensors\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    targets = torch.tensor(np.concatenate([next_states, rewards], axis=1), dtype=torch.float32)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "inputs, targets = collect_random_data(env, steps = 100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfe24f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "sweep...\n",
      "Epoch 0, Loss = 0.4313\n",
      "Epoch 100, Loss = 0.0044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m sweep:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msweep...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     arrays.append(\u001b[43mmodel_run\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# 5, new hidden, 5\u001b[39;00m\n\u001b[32m     70\u001b[39m plot_arrays(arrays, sweep)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mmodel_run\u001b[39m\u001b[34m(in_dim, h_dim, out_dim, x_train, y_train, x_test, y_test)\u001b[39m\n\u001b[32m     38\u001b[39m outputs = model(inputs)\n\u001b[32m     39\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m optimizer.step()\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def plot_arrays(losses, sweep):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    ax.plot(sweep, losses)\n",
    "    ax.set_title(\"loss over cap.\")\n",
    "    ax.set_ylabel(\"loss\")\n",
    "    ax.set_xlabel(\"cap.\")\n",
    "    plt.show()\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim=5, h_dim=64, out_dim=5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, h_dim) # -> s, a\n",
    "        self.h1 = nn.Linear(h_dim, h_dim)\n",
    "        # self.h2 = nn.Linear(64, 64)\n",
    "        # self.h3 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(h_dim, out_dim) # -> s_new, r\n",
    "\n",
    "    def forward(self, input):\n",
    "        res = torch.relu(self.fc1(input))\n",
    "        res = torch.relu(self.h1(res))\n",
    "        res = self.fc2(res)\n",
    "        return res\n",
    "\n",
    "def model_run(in_dim, h_dim, out_dim, x_train, y_train, x_test, y_test):\n",
    "    model = Network(in_dim, h_dim, out_dim).to(device)\n",
    "    criterion = nn.MSELoss()        # or nn.CrossEntropyLoss if classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    inputs = x_train\n",
    "    targets = y_train\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    outputs = model(x_test)\n",
    "    loss = criterion(outputs, y_test)\n",
    "    print(f\"test loss: {loss.item()}\")\n",
    "    return loss.item()\n",
    "\n",
    "sweep =[i * 64 for i in range(1, 10)]\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    inputs,\n",
    "    targets,\n",
    "    test_size=0.2,      # 20% test\n",
    "    random_state=42,    # reproducible split\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "arrays = []\n",
    "\n",
    "for val in sweep:\n",
    "    print(\"sweep...\")\n",
    "    arrays.append(model_run(5, val, 5, x_train, y_train, x_test, y_test)) # 5, new hidden, 5\n",
    "\n",
    "plot_arrays(arrays, sweep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
