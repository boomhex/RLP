{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9087d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libGLEW.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libGLEW.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libGLEW.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py in /home3/s5228786/.local/lib/python3.10/site-packages (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home3/s5228786/.local/lib/python3.10/site-packages (from h5py) (2.2.6)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/cvmfs/hpc.rug.nl/versions/2023.01/rocky8/x86_64/intel/icelake/software/Python/3.10.4-GCCcore-11.3.0/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e81ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NN necessities:\n",
    "import torch                # tensors etc.\n",
    "from torch import nn        # nn helpers\n",
    "\n",
    "# import plotting utilities:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import data preprocessing utilities:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path    # paths\n",
    "import h5py                 # for dataset\n",
    "import numpy as np          # vector calc\n",
    "\n",
    "# import type annotations:\n",
    "from typing import List\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2718ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):   # class defining a basic nn\n",
    "\n",
    "    def __init__(self, h_size=200, h_layers=4):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(23, h_size),      # in\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        for i in range(h_layers):\n",
    "            self.model.append(nn.Linear(h_size, h_size))\n",
    "            self.model.append(nn.ReLU())\n",
    "        self.mean_head = nn.Linear(h_size, 18)\n",
    "        self.logvar_head = nn.Linear(h_size, 18)\n",
    "\n",
    "        # bind log-variance to avoid numerical instability\n",
    "        self.max_logvar = nn.Parameter(torch.ones(18) * 0.5)\n",
    "        self.min_logvar = nn.Parameter(torch.ones(18) * -10)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.MSELoss()    # using mean squared error as a loss metric\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        res = self.model(x)\n",
    "        mean = self.mean_head(res)\n",
    "        logvar = self.logvar_head(res)\n",
    "\n",
    "        # clamp log-variance using soft constraints (see MBPO/PETS)\n",
    "        logvar = self.max_logvar - torch.nn.functional.softplus(self.max_logvar - logvar)\n",
    "        logvar = self.min_logvar + torch.nn.functional.softplus(logvar - self.min_logvar)\n",
    "\n",
    "        return torch.stack([mean, logvar])\n",
    "\n",
    "    def nll_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Negative log-likelihood of Gaussian:\n",
    "            NLL = 0.5 * [ logσ² + (y - µ)² / σ² ]\n",
    "        \"\"\"\n",
    "        mean, logvar = self.forward(x)\n",
    "        var = torch.exp(logvar)\n",
    "\n",
    "        nll = 0.5 * ((y - mean)**2 / var + logvar)\n",
    "        return nll.mean()\n",
    "\n",
    "    def train_epoch(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = self.nll_loss(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, x, y, epochs=500, cp=100, surpress=False):\n",
    "        # again split the data to optimize hyperparam on val set, to not leak data.\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "    \n",
    "        test_losses = []\n",
    "        losses = []\n",
    "    \n",
    "        for iter in range(epochs):\n",
    "            # train\n",
    "            iteration_loss = self.train_epoch(x_train, y_train)\n",
    "            losses.append(iteration_loss)\n",
    "\n",
    "            # validate\n",
    "            val_loss = self.validation_loss((x_val, y_val))\n",
    "            test_losses.append(val_loss)\n",
    "    \n",
    "            # print\n",
    "            if not surpress:\n",
    "                if iter and iter % cp == 0:    # update on iteration checkpoints\n",
    "                    print(f\"iteration {iter}/{epochs}, loss = train: {iteration_loss}, val: {val_loss}\")\n",
    "\n",
    "        return losses, test_losses\n",
    "\n",
    "    def validation_loss(self, test_data):\n",
    "        x, y = test_data\n",
    "        loss = self.nll_loss(x, y)\n",
    "        return loss.item()\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73864f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "\n",
    "    def __init__(self, n_networks, lmbda=0.5, hidden_size=10, hidden_layers=4):\n",
    "\n",
    "        self.n_networks = n_networks\n",
    "        self.models = [\n",
    "            Network(h_size=hidden_size, h_layers=hidden_layers) for i in range(n_networks)\n",
    "        ]\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def train(self, x, y, epochs=500):\n",
    "        for model in self.models:\n",
    "            model.train(x, y, epochs=500, surpress=True)\n",
    "\n",
    "    def test(self, x, y):\n",
    "        predictions: torch.Tensor = self.predict(x)    # get the prediction\n",
    "        loss = self.mse(predictions, y)                # calculate the loss\n",
    "        return loss                                         # report the loss\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        predictions: torch.Tensor = torch.stack(           # shape: (self.n_models, 2, N, out_dim)\n",
    "            [model.forward(x) for model in self.models]\n",
    "        )\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, x) -> torch.Tensor:\n",
    "        predictions: torch.Tensor = self.forward(x)               # forward for all models\n",
    "\n",
    "        # process ensemble predictions and conclude on 1 prediction\n",
    "        corrected_prediction: torch.Tensor = self.process_predictions(predictions)\n",
    "\n",
    "        return corrected_prediction\n",
    "\n",
    "    def process_predictions(self, predictions: torch.Tensor) -> torch.Tensor:\n",
    "        # separate means and variances\n",
    "        means: torch.Tensor = predictions[:, 0, :, :]\n",
    "        vars: torch.Tensor = predictions[:, 1, :, :]\n",
    "\n",
    "        # randomly select means\n",
    "        random_mean = self.select_random_mean(means)\n",
    "\n",
    "        # calculate max variance\n",
    "        max_var = self.max_variance(vars)\n",
    "\n",
    "        # correct reward prediction using max variance\n",
    "        corrected_prediction = self.penalize_prediction(random_mean, max_var)\n",
    "\n",
    "        return corrected_prediction\n",
    "\n",
    "    def select_random_mean(self, means: torch.Tensor):\n",
    "        # means of shape: (7, N, 18)\n",
    "        n_nets, n_samples, n_dims = means.shape\n",
    "\n",
    "        # random network index per sample: (N,)\n",
    "        idx = torch.randint(\n",
    "            low=0,\n",
    "            high=n_nets,\n",
    "            size=(n_samples, 1),\n",
    "            device=means.device\n",
    "        )\n",
    "        # reshape for gather: (N, 7, 18)\n",
    "        means_n = torch.permute(means, (1, 0, 2))\n",
    "\n",
    "        # gather expects index shape to match output shape\n",
    "        # -> (N, 1, 18), then squeeze to (N, 18)\n",
    "        idx_g = idx.view(n_samples, 1, 1).expand(-1, 1, n_dims)\n",
    "        out = means_n.gather(dim=1, index=idx_g).squeeze(1)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def max_variance(self, vars: torch.Tensor) -> torch.Tensor:\n",
    "        out = torch.max(vars[:, :, -1], dim=0).values\n",
    "        return out\n",
    "\n",
    "    def penalize_prediction(self,\n",
    "                            mean: torch.Tensor,\n",
    "                            max_var: torch.Tensor):\n",
    "        mean[:, -1] = mean[:, -1] - self.lmbda * max_var\n",
    "        return mean\n",
    "    \n",
    "    def mse(self, y: torch.Tensor, y_hat: torch.Tensor):\n",
    "        mse = (y - y_hat)**2        # mse per prediction\n",
    "        return torch.mean( mse )    # mean mse\n",
    "\n",
    "    def to(self, device):\n",
    "        for model in self.models:                         # move each model to device\n",
    "            model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c63b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actions', 'infos', 'metadata', 'next_observations', 'observations', 'rewards', 'terminals', 'timeouts']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = h5py.File(Path(\"./halfcheetah_medium-v2.hdf5\"))\n",
    "print([key for key in data.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b8db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape = (1000000, 6)\n",
      "s shape = (1000000, 17)\n",
      "s_new shape = (1000000, 17)\n",
      "r shape = (1000000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract relevant cols\n",
    "a = data[\"actions\"]\n",
    "s_new = data[\"next_observations\"]\n",
    "s = data[\"observations\"]\n",
    "r = data[\"rewards\"]\n",
    "\n",
    "# info\n",
    "print(\n",
    "    f\"a shape = {a.shape}\\n\" \\\n",
    "    f\"s shape = {s.shape}\\n\" \\\n",
    "    f\"s_new shape = {s_new.shape}\\n\" \\\n",
    "    f\"r shape = {r.shape}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21844542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "x_train shape = torch.Size([800000, 23])\n",
      "x_test shape = torch.Size([200000, 23])\n",
      "y_train shape = torch.Size([800000, 18])\n",
      "y_test shape = torch.Size([200000, 18])\n"
     ]
    }
   ],
   "source": [
    "# divide data\n",
    "x = np.hstack([a, s])                                # -> (N, 23)\n",
    "y = np.hstack([s_new, np.array(r).reshape(-1, 1)])   # -> (N, 18)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# converting to tensors\n",
    "x = torch.tensor(x, dtype=torch.float32).to(device)   \n",
    "y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, shuffle=True\n",
    ")\n",
    "\n",
    "# info\n",
    "print(\n",
    "    f\"x_train shape = {x_train.shape}\\n\" \\\n",
    "    f\"x_test shape = {x_test.shape}\\n\" \\\n",
    "    f\"y_train shape = {y_train.shape}\\n\" \\\n",
    "    f\"y_test shape = {y_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "# set params\n",
    "epochs = 100\n",
    "\n",
    "# for network_width in width_list:\n",
    "model = Network( h_size=10 )\n",
    "model = model.to(device)\n",
    "\n",
    "# train\n",
    "train_losses, test_losses = model.train(train_data, epochs=epochs, cp=10)\n",
    "# test_results.append(test_losses)\n",
    "test_loss = model.nll_loss(x_test, y_test)\n",
    "print(f\"final test loss = {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cad589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.arange(0, epochs)\n",
    "ax.plot(x, train_losses, label='train', color='red')\n",
    "ax.plot(x, test_losses, label='test', color='green')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed59497",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m ensemble = Ensemble(\n\u001b[32m      4\u001b[39m     n_networks=\u001b[32m7\u001b[39m, \n\u001b[32m      5\u001b[39m     lmbda=\u001b[32m0.5\u001b[39m,\n\u001b[32m      6\u001b[39m     hidden_size=\u001b[32m10\u001b[39m)\n\u001b[32m      8\u001b[39m ensemble.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mensemble\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m ensemble.test(x_test, y_test)    \u001b[38;5;66;03m# test_data.size: (200_000, 18)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mEnsemble.train\u001b[39m\u001b[34m(self, x, y, epochs)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, epochs=\u001b[32m500\u001b[39m):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurpress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mNetwork.train\u001b[39m\u001b[34m(self, x, y, epochs, cp, surpress)\u001b[39m\n\u001b[32m     58\u001b[39m losses = []\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     iteration_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     losses.append(iteration_loss)\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mNetwork.train_epoch\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     loss.backward()\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mNetwork.nll_loss\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnll_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y):\n\u001b[32m     34\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    Negative log-likelihood of Gaussian:\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m        NLL = 0.5 * [ logσ² + (y - µ)² / σ² ]\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     mean, logvar = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     var = torch.exp(logvar)\n\u001b[32m     41\u001b[39m     nll = \u001b[32m0.5\u001b[39m * ((y - mean)**\u001b[32m2\u001b[39m / var + logvar)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mNetwork.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     28\u001b[39m logvar = \u001b[38;5;28mself\u001b[39m.max_logvar - torch.nn.functional.softplus(\u001b[38;5;28mself\u001b[39m.max_logvar - logvar)\n\u001b[32m     29\u001b[39m logvar = \u001b[38;5;28mself\u001b[39m.min_logvar + torch.nn.functional.softplus(logvar - \u001b[38;5;28mself\u001b[39m.min_logvar)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train ensemble\n",
    "\n",
    "ensemble = Ensemble(\n",
    "    n_networks=7, \n",
    "    lmbda=0.5,\n",
    "    hidden_size=10)\n",
    "\n",
    "ensemble.to(device)\n",
    "\n",
    "ensemble.train(x_train, y_train, epochs=100)\n",
    "\n",
    "ensemble.test(x_test, y_test)    # test_data.size: (200_000, 18)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
